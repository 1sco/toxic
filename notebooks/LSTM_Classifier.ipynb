{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print = tqdm.write\n",
    "tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('..', 'data')\n",
    "train_path = Path(data_path, 'normalized_train.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "train_df['comment_text'] = train_df['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1,\n",
    "                                    random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12579</th>\n",
       "      <td>21590fdb69ac401f</td>\n",
       "      <td>\"\\n\\n Image:Taiwan straits.jpg listed for deletion \\n\\n  An image or media file that you uploaded, Image:Taiwan straits.jpg, has been listed at Wikipedia:Images and media for deletion. Please look there to see why this is (you may have to search for the title of the image to find its entry), if you are interested in it not being deleted. Thank you.   \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95239</th>\n",
       "      <td>fea00ca779850150</td>\n",
       "      <td>\"\\n\\n Talkback Section \\nResponded at my talk. (talk) \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18336</th>\n",
       "      <td>30662d209bf28e5a</td>\n",
       "      <td>Thanks... this is a vast improvement ) The main problem is that I just haven't had time to complete all the states... I'm working away at plugging the gaps bit by bit but it's going to take a while.  Not exactly sure what to do in the meantime.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>0e70afb9cc4b0ac2</td>\n",
       "      <td>Thanks for taking care of that.  Another sockpuppet,</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43579</th>\n",
       "      <td>7455193dec3fcde6</td>\n",
       "      <td>Kathleen Andrews Submission \\n\\nHi Ritchie###, \\n\\nI would like to thank you for helping me submit Kathleen Andrews legacy to Wikipedia. Wow!!!!! You Made my day as I've been trying for a very long time!! THANK YOU!!!\\n\\nIm very new to Wikipedia, so I am not sure how to do this but in Kathleen Andrews story, it listed her as First Female Dispatcher, and thats true, but she was also the First Female ETS Bus Operator, First Female Dispatcher and First Female in ETS Management. The emphasis on the First female Bus Operator should be first, if that's possible.\\n\\nI look forward to hearing from you and a BIG THANK YOU for all of your help!!!\\n\\nTake care\\n\\nLisa Andrews</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141819</th>\n",
       "      <td>f6b8aaa4bf61346c</td>\n",
       "      <td>I am not doing any disruptive changes! The sources you have used are not valid!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117134</th>\n",
       "      <td>71f2b4b004e2ee9e</td>\n",
       "      <td>So how long will this one last and who am i going ot have ot degrade myself by begging to this time?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147217</th>\n",
       "      <td>38c9a5938084661f</td>\n",
       "      <td>\"\\nDo you mean \"\"which was sufficient\"\"?  I'm confused.    \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44413</th>\n",
       "      <td>769cae54627f1fbe</td>\n",
       "      <td>\"\\n Removing  is probably fine.  As for the , this is used in  and would break the only method for adding a reference to the coordinates, so I would leave it.  With regards to the loading times, I didn't think it was any worse than anything else. It could be due to the recent slowness with the servers.  ―Œ(talk) \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>80399abe839de1d7</td>\n",
       "      <td>\"\\n\\nI've changed it to the \"\"neutrality\"\" tag as we all seem to agree that this is the problem with the article.  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  \\\n",
       "12579   21590fdb69ac401f   \n",
       "95239   fea00ca779850150   \n",
       "18336   30662d209bf28e5a   \n",
       "5418    0e70afb9cc4b0ac2   \n",
       "43579   7455193dec3fcde6   \n",
       "141819  f6b8aaa4bf61346c   \n",
       "117134  71f2b4b004e2ee9e   \n",
       "147217  38c9a5938084661f   \n",
       "44413   769cae54627f1fbe   \n",
       "47996   80399abe839de1d7   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             comment_text  \\\n",
       "12579   \"\\n\\n Image:Taiwan straits.jpg listed for deletion \\n\\n  An image or media file that you uploaded, Image:Taiwan straits.jpg, has been listed at Wikipedia:Images and media for deletion. Please look there to see why this is (you may have to search for the title of the image to find its entry), if you are interested in it not being deleted. Thank you.   \"                                                                                                                                                                                                                                                                                                                                  \n",
       "95239   \"\\n\\n Talkback Section \\nResponded at my talk. (talk) \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "18336   Thanks... this is a vast improvement ) The main problem is that I just haven't had time to complete all the states... I'm working away at plugging the gaps bit by bit but it's going to take a while.  Not exactly sure what to do in the meantime.                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "5418    Thanks for taking care of that.  Another sockpuppet,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "43579   Kathleen Andrews Submission \\n\\nHi Ritchie###, \\n\\nI would like to thank you for helping me submit Kathleen Andrews legacy to Wikipedia. Wow!!!!! You Made my day as I've been trying for a very long time!! THANK YOU!!!\\n\\nIm very new to Wikipedia, so I am not sure how to do this but in Kathleen Andrews story, it listed her as First Female Dispatcher, and thats true, but she was also the First Female ETS Bus Operator, First Female Dispatcher and First Female in ETS Management. The emphasis on the First female Bus Operator should be first, if that's possible.\\n\\nI look forward to hearing from you and a BIG THANK YOU for all of your help!!!\\n\\nTake care\\n\\nLisa Andrews   \n",
       "141819  I am not doing any disruptive changes! The sources you have used are not valid!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "117134  So how long will this one last and who am i going ot have ot degrade myself by begging to this time?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "147217  \"\\nDo you mean \"\"which was sufficient\"\"?  I'm confused.    \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "44413   \"\\n Removing  is probably fine.  As for the , this is used in  and would break the only method for adding a reference to the coordinates, so I would leave it.  With regards to the loading times, I didn't think it was any worse than anything else. It could be due to the recent slowness with the servers.  ―Œ(talk) \"                                                                                                                                                                                                                                                                                                                                                                         \n",
       "47996   \"\\n\\nI've changed it to the \"\"neutrality\"\" tag as we all seem to agree that this is the problem with the article.  \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "12579   0      0             0        0       0       0              \n",
       "95239   0      0             0        0       0       0              \n",
       "18336   0      0             0        0       0       0              \n",
       "5418    0      0             0        0       0       0              \n",
       "43579   0      0             0        0       0       0              \n",
       "141819  0      0             0        0       0       0              \n",
       "117134  0      0             0        0       0       0              \n",
       "147217  0      0             0        0       0       0              \n",
       "44413   0      0             0        0       0       0              \n",
       "47996   0      0             0        0       0       0              "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', -1)\n",
    "train_df.sample(10, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_examples(df: pd.DataFrame, fields):\n",
    "    fields = {field_name: (field_name, field)\n",
    "                       for field_name, field in fields.items()}\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        example = data.Example.fromdict(row, fields)\n",
    "        yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143613it [03:32, 675.19it/s]\n",
      "15958it [00:26, 612.60it/s]\n"
     ]
    }
   ],
   "source": [
    "text_field = data.Field(init_token='<START>', eos_token='<EOS>',\n",
    "                        lower=True, tokenize=nltk.word_tokenize,\n",
    "                        include_lengths=True, batch_first=True)\n",
    "label_field = data.Field(sequential=False, use_vocab=False,\n",
    "                         tensor_type=torch.FloatTensor)\n",
    "\n",
    "# fields = [('id', None), ('comment_text', text_field),\n",
    "#           ('toxic', label_field), ('severe_toxic', label_field),\n",
    "#           ('obscene', label_field), ('threat', label_field),\n",
    "#           ('insult', label_field), ('identity_hate', label_field)]\n",
    "\n",
    "# train_set = data.TabularDataset(str(train_path), fields=fields,\n",
    "#                                 skip_header=True, format='csv')\n",
    "\n",
    "fields = OrderedDict({'comment_text': text_field, 'toxic': label_field,\n",
    "                      'severe_toxic': label_field, 'obscene': label_field,\n",
    "                      'threat': label_field, 'insult': label_field,\n",
    "                      'identity_hate': label_field})\n",
    "\n",
    "\n",
    "train_examples = list(make_train_examples(train_df, fields))\n",
    "val_examples = list(make_train_examples(val_df, fields))\n",
    "\n",
    "train_set = data.Dataset(train_examples, fields)\n",
    "val_set = data.Dataset(val_examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field.build_vocab(train_set, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, *, n_classes, vocab_size,\n",
    "                 num_hidden=256, padding_idx=None,\n",
    "                 embedding_dim=100, embedding_weights=None,\n",
    "                 bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, \n",
    "                                  embedding_dim=embedding_dim,\n",
    "                                  padding_idx=padding_idx)\n",
    "        if embedding_weights is not None:\n",
    "            self.embed.weight.data.copy_(embedding_weights)\n",
    "            \n",
    "        self.lstm = nn.LSTM(embedding_dim, num_hidden,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.out = nn.Linear(num_hidden * 2, n_classes)\n",
    "    \n",
    "    def forward(self, X, lengths):\n",
    "        X = self.embed(X)\n",
    "        X = nn.utils.rnn.pack_padded_sequence(X, lengths,\n",
    "                                              batch_first=True)\n",
    "        X, _ = self.lstm(X)\n",
    "        X, lengths = nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "        # take the last time step\n",
    "        X = X[:, -1, :]\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notnami\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:116: UserWarning: \n",
      "    Found GPU0 GeForce GPU which is of cuda capability 5.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6b0a4eb087e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                      padding_idx=padding_idx)\n\u001b[0;32m     10\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(cls, s, file, end, nolock)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal_write_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnolock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m# Write the message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    161\u001b[0m         '''\n\u001b[0;32m    162\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_osc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANSI_CSI_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mconvert_osc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconvert_osc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mANSI_OSC_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m             \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "label_fields = ['toxic', 'severe_toxic', 'obscene',\n",
    "                'threat', 'insult', 'identity_hate']\n",
    "padding_idx = text_field.vocab.stoi[text_field.pad_token]\n",
    "\n",
    "\n",
    "\n",
    "net = TextClassifier(n_classes=len(label_fields), \n",
    "                     vocab_size=len(text_field.vocab),\n",
    "                     padding_idx=padding_idx)\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84118c8cd9464e2581d1d7947db3f039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 2.598\n",
      "[1, 200] loss: 2.163\n",
      "[1, 300] loss: 2.149\n",
      "[1, 400] loss: 1.928\n",
      "[1, 500] loss: 1.747\n",
      "[1, 600] loss: 1.816\n",
      "[1, 700] loss: 1.811\n",
      "[1, 800] loss: 1.722\n",
      "[1, 900] loss: 1.670\n",
      "[1, 1000] loss: 1.620\n",
      "[1, 1100] loss: 1.753\n",
      "[1, 1200] loss: 1.439\n",
      "[1, 1300] loss: 1.439\n",
      "[1, 1400] loss: 1.424\n",
      "[1, 1500] loss: 1.281\n",
      "[1, 1600] loss: 1.306\n",
      "[1, 1700] loss: 1.399\n",
      "[1, 1800] loss: 1.345\n",
      "[1, 1900] loss: 1.191\n",
      "[1, 2000] loss: 1.224\n",
      "[1, 2100] loss: 1.235\n",
      "[1, 2200] loss: 1.191\n",
      "[1, 2300] loss: 1.176\n",
      "[1, 2400] loss: 1.128\n",
      "[1, 2500] loss: 1.192\n",
      "[1, 2600] loss: 0.926\n",
      "[1, 2700] loss: 1.033\n",
      "[1, 2800] loss: 0.915\n",
      "[1, 2900] loss: 0.988\n",
      "[1, 3000] loss: 0.871\n",
      "[1, 3100] loss: 0.932\n",
      "[1, 3200] loss: 0.930\n",
      "[1, 3300] loss: 1.084\n",
      "[1, 3400] loss: 0.853\n",
      "[1, 3500] loss: 0.839\n",
      "[1, 3600] loss: 0.895\n",
      "[1, 3700] loss: 0.834\n",
      "[1, 3800] loss: 0.876\n",
      "[1, 3900] loss: 0.777\n",
      "[1, 4000] loss: 0.880\n",
      "[1, 4100] loss: 0.819\n",
      "[1, 4200] loss: 0.844\n",
      "[1, 4300] loss: 0.743\n",
      "[1, 4400] loss: 0.821\n",
      "[1, 4500] loss: 0.873\n",
      "[1, 4600] loss: 0.791\n",
      "[1, 4700] loss: 0.685\n",
      "[1, 4800] loss: 0.768\n",
      "[1, 4900] loss: 0.649\n",
      "[1, 5000] loss: 0.657\n",
      "[1, 5100] loss: 0.708\n",
      "[1, 5200] loss: 0.660\n",
      "[1, 5300] loss: 0.724\n",
      "[1, 5400] loss: 0.666\n",
      "[1, 5500] loss: 0.665\n",
      "[1, 5600] loss: 0.708\n",
      "[1, 5700] loss: 0.817\n",
      "[1, 5800] loss: 0.865\n",
      "[1, 5900] loss: 0.753\n",
      "[1, 6000] loss: 0.668\n",
      "[1, 6100] loss: 0.667\n",
      "[1, 6200] loss: 0.619\n",
      "[1, 6300] loss: 0.568\n",
      "[1, 6400] loss: 0.566\n",
      "[1, 6500] loss: 0.557\n",
      "[1, 6600] loss: 0.588\n",
      "[1, 6700] loss: 0.620\n",
      "[1, 6800] loss: 0.671\n",
      "[1, 6900] loss: 0.547\n",
      "[1, 7000] loss: 0.566\n",
      "[1, 7100] loss: 0.629\n",
      "[1, 7200] loss: 0.553\n",
      "[1, 7300] loss: 0.600\n",
      "[1, 7400] loss: 0.728\n",
      "[1, 7500] loss: 0.668\n",
      "[1, 7600] loss: 0.616\n",
      "[1, 7700] loss: 0.458\n",
      "[1, 7800] loss: 0.578\n",
      "[1, 7900] loss: 0.656\n",
      "[1, 8000] loss: 0.733\n",
      "[1, 8100] loss: 0.592\n",
      "[1, 8200] loss: 0.700\n",
      "[1, 8300] loss: 0.549\n",
      "[1, 8400] loss: 0.712\n",
      "[1, 8500] loss: 0.584\n",
      "[1, 8600] loss: 0.648\n",
      "[1, 8700] loss: 0.762\n",
      "[1, 8800] loss: 0.582\n",
      "[1, 8900] loss: 0.671\n",
      "[1, 9000] loss: 0.564\n",
      "[1, 9100] loss: 0.674\n",
      "[1, 9200] loss: 0.499\n",
      "[1, 9300] loss: 0.621\n",
      "[1, 9400] loss: 0.465\n",
      "[1, 9500] loss: 0.481\n",
      "[1, 9600] loss: 0.645\n",
      "[1, 9700] loss: 0.478\n",
      "[1, 9800] loss: 0.567\n",
      "[1, 9900] loss: 0.559\n",
      "[1, 10000] loss: 0.529\n",
      "[1, 10100] loss: 0.679\n",
      "[1, 10200] loss: 0.570\n",
      "[1, 10300] loss: 0.836\n",
      "[1, 10400] loss: 0.488\n",
      "[1, 10500] loss: 0.553\n",
      "[1, 10600] loss: 0.574\n",
      "[1, 10700] loss: 0.609\n",
      "[1, 10800] loss: 0.601\n",
      "[1, 10900] loss: 0.635\n",
      "[1, 11000] loss: 0.406\n",
      "[1, 11100] loss: 0.524\n",
      "[1, 11200] loss: 0.535\n",
      "[1, 11300] loss: 0.668\n",
      "[1, 11400] loss: 0.583\n",
      "[1, 11500] loss: 0.461\n",
      "[1, 11600] loss: 0.458\n",
      "[1, 11700] loss: 0.556\n",
      "[1, 11800] loss: 0.496\n",
      "[1, 11900] loss: 0.457\n",
      "[1, 12000] loss: 0.451\n",
      "[1, 12100] loss: 0.477\n",
      "[1, 12200] loss: 0.531\n",
      "[1, 12300] loss: 0.614\n",
      "[1, 12400] loss: 0.672\n",
      "[1, 12500] loss: 0.598\n",
      "[1, 12600] loss: 0.710\n",
      "[1, 12700] loss: 0.530\n",
      "[1, 12800] loss: 0.565\n",
      "[1, 12900] loss: 0.572\n",
      "[1, 13000] loss: 0.644\n",
      "[1, 13100] loss: 0.566\n",
      "[1, 13200] loss: 0.449\n",
      "[1, 13300] loss: 0.429\n",
      "[1, 13400] loss: 0.569\n",
      "[1, 13500] loss: 0.519\n",
      "[1, 13600] loss: 0.484\n",
      "[1, 13700] loss: 0.741\n",
      "[1, 13800] loss: 0.457\n",
      "[1, 13900] loss: 0.576\n",
      "[1, 14000] loss: 0.503\n",
      "[1, 14100] loss: 0.457\n",
      "[1, 14200] loss: 0.455\n",
      "[1, 14300] loss: 0.526\n",
      "[1, 14400] loss: 0.459\n",
      "[1, 14500] loss: 0.505\n",
      "[1, 14600] loss: 0.498\n",
      "[1, 14700] loss: 0.423\n",
      "[1, 14800] loss: 0.452\n",
      "[1, 14900] loss: 0.550\n",
      "[1, 15000] loss: 0.565\n",
      "[1, 15100] loss: 0.787\n",
      "[1, 15200] loss: 0.597\n",
      "[1, 15300] loss: 0.504\n",
      "[1, 15400] loss: 0.427\n",
      "[1, 15500] loss: 0.604\n",
      "[1, 15600] loss: 0.556\n",
      "[1, 15700] loss: 0.680\n",
      "[1, 15800] loss: 0.509\n",
      "[1, 15900] loss: 0.558\n",
      "[1, 16000] loss: 0.515\n",
      "[1, 16100] loss: 0.585\n",
      "[1, 16200] loss: 0.525\n",
      "[1, 16300] loss: 0.517\n",
      "[1, 16400] loss: 0.693\n",
      "[1, 16500] loss: 0.491\n",
      "[1, 16600] loss: 0.473\n",
      "[1, 16700] loss: 0.541\n",
      "[1, 16800] loss: 0.553\n",
      "[1, 16900] loss: 0.582\n",
      "[1, 17000] loss: 0.437\n",
      "[1, 17100] loss: 0.487\n",
      "[1, 17200] loss: 0.492\n",
      "[1, 17300] loss: 0.563\n",
      "[1, 17400] loss: 0.484\n",
      "[1, 17500] loss: 0.547\n",
      "[1, 17600] loss: 0.509\n",
      "[1, 17700] loss: 0.509\n",
      "[1, 17800] loss: 0.665\n",
      "[1, 17900] loss: 0.517\n",
      "[1, 18000] loss: 0.522\n",
      "[1, 18100] loss: 0.571\n",
      "[1, 18200] loss: 0.519\n",
      "[1, 18300] loss: 0.605\n",
      "[1, 18400] loss: 0.578\n",
      "[1, 18500] loss: 0.558\n",
      "[1, 18600] loss: 0.529\n",
      "[1, 18700] loss: 0.592\n",
      "[1, 18800] loss: 0.515\n",
      "[1, 18900] loss: 0.523\n",
      "[1, 19000] loss: 0.416\n",
      "[1, 19100] loss: 0.524\n",
      "[1, 19200] loss: 0.667\n",
      "[1, 19300] loss: 0.598\n",
      "[1, 19400] loss: 0.455\n",
      "[1, 19500] loss: 0.434\n",
      "[1, 19600] loss: 0.490\n",
      "[1, 19700] loss: 0.601\n",
      "[1, 19800] loss: 0.594\n",
      "[1, 19900] loss: 0.477\n",
      "[1, 20000] loss: 0.487\n",
      "[1, 20100] loss: 0.650\n",
      "[1, 20200] loss: 0.650\n",
      "[1, 20300] loss: 0.574\n",
      "[1, 20400] loss: 0.657\n",
      "[1, 20500] loss: 0.526\n",
      "[1, 20600] loss: 0.508\n",
      "[1, 20700] loss: 0.626\n",
      "[1, 20800] loss: 0.663\n",
      "[1, 20900] loss: 0.491\n",
      "[1, 21000] loss: 0.508\n",
      "[1, 21100] loss: 0.447\n",
      "[1, 21200] loss: 0.510\n",
      "[1, 21300] loss: 0.544\n",
      "[1, 21400] loss: 0.478\n",
      "[1, 21500] loss: 0.733\n",
      "[1, 21600] loss: 0.491\n",
      "[1, 21700] loss: 0.541\n",
      "[1, 21800] loss: 0.526\n",
      "[1, 21900] loss: 0.432\n",
      "[1, 22000] loss: 0.447\n",
      "[1, 22100] loss: 0.530\n",
      "[1, 22200] loss: 0.544\n",
      "[1, 22300] loss: 0.473\n",
      "[1, 22400] loss: 0.538\n",
      "[1, 22500] loss: 0.576\n",
      "[1, 22600] loss: 0.547\n",
      "[1, 22700] loss: 0.633\n",
      "[1, 22800] loss: 0.387\n",
      "[1, 22900] loss: 0.493\n",
      "[1, 23000] loss: 0.637\n",
      "[1, 23100] loss: 0.644\n",
      "[1, 23200] loss: 0.626\n",
      "[1, 23300] loss: 0.676\n",
      "[1, 23400] loss: 0.584\n",
      "[1, 23500] loss: 0.562\n",
      "[1, 23600] loss: 0.539\n",
      "[1, 23700] loss: 0.554\n",
      "[1, 23800] loss: 0.542\n",
      "[1, 23900] loss: 0.748\n",
      "[1, 24000] loss: 0.546\n",
      "[1, 24100] loss: 0.451\n",
      "[1, 24200] loss: 0.456\n",
      "[1, 24300] loss: 0.547\n",
      "[1, 24400] loss: 0.412\n",
      "[1, 24500] loss: 0.529\n",
      "[1, 24600] loss: 0.572\n",
      "[1, 24700] loss: 0.404\n",
      "[1, 24800] loss: 0.407\n",
      "[1, 24900] loss: 0.378\n",
      "[1, 25000] loss: 0.529\n",
      "[1, 25100] loss: 0.533\n",
      "[1, 25200] loss: 0.568\n",
      "[1, 25300] loss: 0.689\n",
      "[1, 25400] loss: 0.474\n",
      "[1, 25500] loss: 0.562\n",
      "[1, 25600] loss: 0.358\n",
      "[1, 25700] loss: 0.486\n",
      "[1, 25800] loss: 0.396\n",
      "[1, 25900] loss: 0.531\n",
      "[1, 26000] loss: 0.562\n",
      "[1, 26100] loss: 0.528\n",
      "[1, 26200] loss: 0.454\n",
      "[1, 26300] loss: 0.572\n",
      "[1, 26400] loss: 0.460\n",
      "[1, 26500] loss: 0.476\n",
      "[1, 26600] loss: 0.636\n",
      "[1, 26700] loss: 0.468\n",
      "[1, 26800] loss: 0.666\n",
      "[1, 26900] loss: 0.452\n",
      "[1, 27000] loss: 0.518\n",
      "[1, 27100] loss: 0.714\n",
      "[1, 27200] loss: 0.669\n",
      "[1, 27300] loss: 0.425\n",
      "[1, 27400] loss: 0.620\n",
      "[1, 27500] loss: 0.483\n",
      "[1, 27600] loss: 0.598\n",
      "[1, 27700] loss: 0.436\n",
      "[1, 27800] loss: 0.634\n",
      "[1, 27900] loss: 0.476\n",
      "[1, 28000] loss: 0.576\n",
      "[1, 28100] loss: 0.478\n",
      "[1, 28200] loss: 0.498\n",
      "[1, 28300] loss: 0.502\n",
      "[1, 28400] loss: 0.473\n",
      "[1, 28500] loss: 0.486\n",
      "[1, 28600] loss: 0.690\n",
      "[1, 28700] loss: 0.394\n",
      "[1, 28800] loss: 0.509\n",
      "[1, 28900] loss: 0.596\n",
      "[1, 29000] loss: 0.487\n",
      "[1, 29100] loss: 0.594\n",
      "[1, 29200] loss: 0.588\n",
      "[1, 29300] loss: 0.512\n",
      "[1, 29400] loss: 0.525\n",
      "[1, 29500] loss: 0.555\n",
      "[1, 29600] loss: 0.436\n",
      "[1, 29700] loss: 0.510\n",
      "[1, 29800] loss: 0.560\n",
      "[1, 29900] loss: 0.649\n",
      "[1, 30000] loss: 0.519\n",
      "[1, 30100] loss: 0.524\n",
      "[1, 30200] loss: 0.497\n",
      "[1, 30300] loss: 0.513\n",
      "[1, 30400] loss: 0.508\n",
      "[1, 30500] loss: 0.443\n",
      "[1, 30600] loss: 0.498\n",
      "[1, 30700] loss: 0.535\n",
      "[1, 30800] loss: 0.608\n",
      "[1, 30900] loss: 0.591\n",
      "[1, 31000] loss: 0.542\n",
      "[1, 31100] loss: 0.282\n",
      "[1, 31200] loss: 0.507\n",
      "[1, 31300] loss: 0.461\n",
      "[1, 31400] loss: 0.438\n",
      "[1, 31500] loss: 0.524\n",
      "[1, 31600] loss: 0.487\n",
      "[1, 31700] loss: 0.629\n",
      "[1, 31800] loss: 0.518\n",
      "[1, 31900] loss: 0.404\n",
      "[1, 32000] loss: 0.493\n",
      "[1, 32100] loss: 0.440\n",
      "[1, 32200] loss: 0.571\n",
      "[1, 32300] loss: 0.615\n",
      "[1, 32400] loss: 0.591\n",
      "[1, 32500] loss: 0.571\n",
      "[1, 32600] loss: 0.562\n",
      "[1, 32700] loss: 0.521\n",
      "[1, 32800] loss: 0.515\n",
      "[1, 32900] loss: 0.622\n",
      "[1, 33000] loss: 0.554\n",
      "[1, 33100] loss: 0.500\n",
      "[1, 33200] loss: 0.488\n",
      "[1, 33300] loss: 0.490\n",
      "[1, 33400] loss: 0.590\n",
      "[1, 33500] loss: 0.472\n",
      "[1, 33600] loss: 0.512\n",
      "[1, 33700] loss: 0.492\n",
      "[1, 33800] loss: 0.458\n",
      "[1, 33900] loss: 0.398\n",
      "[1, 34000] loss: 0.603\n",
      "[1, 34100] loss: 0.542\n",
      "[1, 34200] loss: 0.721\n",
      "[1, 34300] loss: 0.563\n",
      "[1, 34400] loss: 0.707\n",
      "[1, 34500] loss: 0.501\n",
      "[1, 34600] loss: 0.485\n",
      "[1, 34700] loss: 0.556\n",
      "[1, 34800] loss: 0.464\n",
      "[1, 34900] loss: 0.648\n",
      "[1, 35000] loss: 0.642\n",
      "[1, 35100] loss: 0.512\n",
      "[1, 35200] loss: 0.597\n",
      "[1, 35300] loss: 0.371\n",
      "[1, 35400] loss: 0.491\n",
      "[1, 35500] loss: 0.623\n",
      "[1, 35600] loss: 0.549\n",
      "[1, 35700] loss: 0.485\n",
      "[1, 35800] loss: 0.503\n",
      "[1, 35900] loss: 0.535\n",
      "[1, 36000] loss: 0.573\n",
      "[1, 36100] loss: 0.716\n",
      "[1, 36200] loss: 0.562\n",
      "[1, 36300] loss: 0.563\n",
      "[1, 36400] loss: 0.581\n",
      "[1, 36500] loss: 0.409\n",
      "[1, 36600] loss: 0.474\n",
      "[1, 36700] loss: 0.435\n",
      "[1, 36800] loss: 0.507\n",
      "[1, 36900] loss: 0.463\n",
      "[1, 37000] loss: 0.395\n",
      "[1, 37100] loss: 0.496\n",
      "[1, 37200] loss: 0.667\n",
      "[1, 37300] loss: 0.563\n",
      "[1, 37400] loss: 0.482\n",
      "[1, 37500] loss: 0.457\n",
      "[1, 37600] loss: 0.574\n",
      "[1, 37700] loss: 0.421\n",
      "[1, 37800] loss: 0.517\n",
      "[1, 37900] loss: 0.387\n",
      "[1, 38000] loss: 0.353\n",
      "[1, 38100] loss: 0.464\n",
      "[1, 38200] loss: 0.533\n",
      "[1, 38300] loss: 0.507\n",
      "[1, 38400] loss: 0.501\n",
      "[1, 38500] loss: 0.469\n",
      "[1, 38600] loss: 0.439\n",
      "[1, 38700] loss: 0.501\n",
      "[1, 38800] loss: 0.490\n",
      "[1, 38900] loss: 0.617\n",
      "[1, 39000] loss: 0.541\n",
      "[1, 39100] loss: 0.542\n",
      "[1, 39200] loss: 0.349\n",
      "[1, 39300] loss: 0.641\n",
      "[1, 39400] loss: 0.418\n",
      "[1, 39500] loss: 0.435\n",
      "[1, 39600] loss: 0.611\n",
      "[1, 39700] loss: 0.449\n",
      "[1, 39800] loss: 0.532\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710e9ca4dfa0458481f845394e326194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 100] loss: 0.556\n",
      "[2, 200] loss: 0.444\n",
      "[2, 300] loss: 0.431\n",
      "[2, 400] loss: 0.445\n",
      "[2, 500] loss: 0.635\n",
      "[2, 600] loss: 0.558\n",
      "[2, 700] loss: 0.410\n",
      "[2, 800] loss: 0.489\n",
      "[2, 900] loss: 0.479\n",
      "[2, 1000] loss: 0.589\n",
      "[2, 1100] loss: 0.554\n",
      "[2, 1200] loss: 0.546\n",
      "[2, 1300] loss: 0.439\n",
      "[2, 1400] loss: 0.590\n",
      "[2, 1500] loss: 0.546\n",
      "[2, 1600] loss: 0.453\n",
      "[2, 1700] loss: 0.546\n",
      "[2, 1800] loss: 0.608\n",
      "[2, 1900] loss: 0.480\n",
      "[2, 2000] loss: 0.456\n",
      "[2, 2100] loss: 0.575\n",
      "[2, 2200] loss: 0.404\n",
      "[2, 2300] loss: 0.616\n",
      "[2, 2400] loss: 0.490\n",
      "[2, 2500] loss: 0.365\n",
      "[2, 2600] loss: 0.618\n",
      "[2, 2700] loss: 0.405\n",
      "[2, 2800] loss: 0.605\n",
      "[2, 2900] loss: 0.645\n",
      "[2, 3000] loss: 0.545\n",
      "[2, 3100] loss: 0.413\n",
      "[2, 3200] loss: 0.576\n",
      "[2, 3300] loss: 0.684\n",
      "[2, 3400] loss: 0.559\n",
      "[2, 3500] loss: 0.610\n",
      "[2, 3600] loss: 0.413\n",
      "[2, 3700] loss: 0.364\n",
      "[2, 3800] loss: 0.489\n",
      "[2, 3900] loss: 0.455\n",
      "[2, 4000] loss: 0.502\n",
      "[2, 4100] loss: 0.484\n",
      "[2, 4200] loss: 0.251\n",
      "[2, 4300] loss: 0.494\n",
      "[2, 4400] loss: 0.463\n",
      "[2, 4500] loss: 0.437\n",
      "[2, 4600] loss: 0.329\n",
      "[2, 4700] loss: 0.330\n",
      "[2, 4800] loss: 0.481\n",
      "[2, 4900] loss: 0.405\n",
      "[2, 5000] loss: 0.603\n",
      "[2, 5100] loss: 0.440\n",
      "[2, 5200] loss: 0.553\n",
      "[2, 5300] loss: 0.448\n",
      "[2, 5400] loss: 0.643\n",
      "[2, 5500] loss: 0.449\n",
      "[2, 5600] loss: 0.680\n",
      "[2, 5700] loss: 0.578\n",
      "[2, 5800] loss: 0.597\n",
      "[2, 5900] loss: 0.495\n",
      "[2, 6000] loss: 0.481\n",
      "[2, 6100] loss: 0.496\n",
      "[2, 6200] loss: 0.394\n",
      "[2, 6300] loss: 0.535\n",
      "[2, 6400] loss: 0.591\n",
      "[2, 6500] loss: 0.556\n",
      "[2, 6600] loss: 0.509\n",
      "[2, 6700] loss: 0.444\n",
      "[2, 6800] loss: 0.446\n",
      "[2, 6900] loss: 0.469\n",
      "[2, 7000] loss: 0.491\n",
      "[2, 7100] loss: 0.422\n",
      "[2, 7200] loss: 0.599\n",
      "[2, 7300] loss: 0.445\n",
      "[2, 7400] loss: 0.459\n",
      "[2, 7500] loss: 0.582\n",
      "[2, 7600] loss: 0.537\n",
      "[2, 7700] loss: 0.432\n",
      "[2, 7800] loss: 0.468\n",
      "[2, 7900] loss: 0.495\n",
      "[2, 8000] loss: 0.414\n",
      "[2, 8100] loss: 0.511\n",
      "[2, 8200] loss: 0.557\n",
      "[2, 8300] loss: 0.592\n",
      "[2, 8400] loss: 0.513\n",
      "[2, 8500] loss: 0.528\n",
      "[2, 8600] loss: 0.451\n",
      "[2, 8700] loss: 0.437\n",
      "[2, 8800] loss: 0.531\n",
      "[2, 8900] loss: 0.508\n",
      "[2, 9000] loss: 0.439\n",
      "[2, 9100] loss: 0.467\n",
      "[2, 9200] loss: 0.425\n",
      "[2, 9300] loss: 0.644\n",
      "[2, 9400] loss: 0.508\n",
      "[2, 9500] loss: 0.496\n",
      "[2, 9600] loss: 0.539\n",
      "[2, 9700] loss: 0.366\n",
      "[2, 9800] loss: 0.606\n",
      "[2, 9900] loss: 0.375\n",
      "[2, 10000] loss: 0.499\n",
      "[2, 10100] loss: 0.441\n",
      "[2, 10200] loss: 0.633\n",
      "[2, 10300] loss: 0.449\n",
      "[2, 10400] loss: 0.481\n",
      "[2, 10500] loss: 0.481\n",
      "[2, 10600] loss: 0.439\n",
      "[2, 10700] loss: 0.706\n",
      "[2, 10800] loss: 0.302\n",
      "[2, 10900] loss: 0.439\n",
      "[2, 11000] loss: 0.406\n",
      "[2, 11100] loss: 0.384\n",
      "[2, 11200] loss: 0.509\n",
      "[2, 11300] loss: 0.522\n",
      "[2, 11400] loss: 0.524\n",
      "[2, 11500] loss: 0.510\n",
      "[2, 11600] loss: 0.496\n",
      "[2, 11700] loss: 0.586\n",
      "[2, 11800] loss: 0.446\n",
      "[2, 11900] loss: 0.598\n",
      "[2, 12000] loss: 0.553\n",
      "[2, 12100] loss: 0.468\n",
      "[2, 12200] loss: 0.398\n",
      "[2, 12300] loss: 0.738\n",
      "[2, 12400] loss: 0.459\n",
      "[2, 12500] loss: 0.507\n",
      "[2, 12600] loss: 0.455\n",
      "[2, 12700] loss: 0.426\n",
      "[2, 12800] loss: 0.497\n",
      "[2, 12900] loss: 0.506\n",
      "[2, 13000] loss: 0.471\n",
      "[2, 13100] loss: 0.368\n",
      "[2, 13200] loss: 0.507\n",
      "[2, 13300] loss: 0.463\n",
      "[2, 13400] loss: 0.571\n",
      "[2, 13500] loss: 0.375\n",
      "[2, 13600] loss: 0.505\n",
      "[2, 13700] loss: 0.288\n",
      "[2, 13800] loss: 0.478\n",
      "[2, 13900] loss: 0.409\n",
      "[2, 14000] loss: 0.327\n",
      "[2, 14100] loss: 0.462\n",
      "[2, 14200] loss: 0.513\n",
      "[2, 14300] loss: 0.497\n",
      "[2, 14400] loss: 0.475\n",
      "[2, 14500] loss: 0.504\n",
      "[2, 14600] loss: 0.548\n",
      "[2, 14700] loss: 0.500\n",
      "[2, 14800] loss: 0.524\n",
      "[2, 14900] loss: 0.525\n",
      "[2, 15000] loss: 0.585\n",
      "[2, 15100] loss: 0.457\n",
      "[2, 15200] loss: 0.562\n",
      "[2, 15300] loss: 0.479\n",
      "[2, 15400] loss: 0.539\n",
      "[2, 15500] loss: 0.572\n",
      "[2, 15600] loss: 0.516\n",
      "[2, 15700] loss: 0.499\n",
      "[2, 15800] loss: 0.654\n",
      "[2, 15900] loss: 0.482\n",
      "[2, 16000] loss: 0.777\n",
      "[2, 16100] loss: 0.454\n",
      "[2, 16200] loss: 0.480\n",
      "[2, 16300] loss: 0.537\n",
      "[2, 16400] loss: 0.429\n",
      "[2, 16500] loss: 0.389\n",
      "[2, 16600] loss: 0.483\n",
      "[2, 16700] loss: 0.529\n",
      "[2, 16800] loss: 0.542\n",
      "[2, 16900] loss: 0.529\n",
      "[2, 17000] loss: 0.470\n",
      "[2, 17100] loss: 0.328\n",
      "[2, 17200] loss: 0.517\n",
      "[2, 17300] loss: 0.484\n",
      "[2, 17400] loss: 0.535\n",
      "[2, 17500] loss: 0.509\n",
      "[2, 17600] loss: 0.483\n",
      "[2, 17700] loss: 0.521\n",
      "[2, 17800] loss: 0.476\n",
      "[2, 17900] loss: 0.324\n",
      "[2, 18000] loss: 0.492\n",
      "[2, 18100] loss: 0.533\n",
      "[2, 18200] loss: 0.544\n",
      "[2, 18300] loss: 0.410\n",
      "[2, 18400] loss: 0.511\n",
      "[2, 18500] loss: 0.633\n",
      "[2, 18600] loss: 0.424\n",
      "[2, 18700] loss: 0.541\n",
      "[2, 18800] loss: 0.489\n",
      "[2, 18900] loss: 0.335\n",
      "[2, 19000] loss: 0.570\n",
      "[2, 19100] loss: 0.534\n",
      "[2, 19200] loss: 0.521\n",
      "[2, 19300] loss: 0.372\n",
      "[2, 19400] loss: 0.528\n",
      "[2, 19500] loss: 0.674\n",
      "[2, 19600] loss: 0.479\n",
      "[2, 19700] loss: 0.490\n",
      "[2, 19800] loss: 0.545\n",
      "[2, 19900] loss: 0.557\n",
      "[2, 20000] loss: 0.343\n",
      "[2, 20100] loss: 0.477\n",
      "[2, 20200] loss: 0.436\n",
      "[2, 20300] loss: 0.502\n",
      "[2, 20400] loss: 0.475\n",
      "[2, 20500] loss: 0.603\n",
      "[2, 20600] loss: 0.381\n",
      "[2, 20700] loss: 0.400\n",
      "[2, 20800] loss: 0.504\n",
      "[2, 20900] loss: 0.616\n",
      "[2, 21000] loss: 0.395\n",
      "[2, 21100] loss: 0.412\n",
      "[2, 21200] loss: 0.477\n",
      "[2, 21300] loss: 0.559\n",
      "[2, 21400] loss: 0.422\n",
      "[2, 21500] loss: 0.409\n",
      "[2, 21600] loss: 0.489\n",
      "[2, 21700] loss: 0.458\n",
      "[2, 21800] loss: 0.434\n",
      "[2, 21900] loss: 0.484\n",
      "[2, 22000] loss: 0.425\n",
      "[2, 22100] loss: 0.419\n",
      "[2, 22200] loss: 0.557\n",
      "[2, 22300] loss: 0.382\n",
      "[2, 22400] loss: 0.499\n",
      "[2, 22500] loss: 0.280\n",
      "[2, 22600] loss: 0.399\n",
      "[2, 22700] loss: 0.582\n",
      "[2, 22800] loss: 0.351\n",
      "[2, 22900] loss: 0.473\n",
      "[2, 23000] loss: 0.404\n",
      "[2, 23100] loss: 0.662\n",
      "[2, 23200] loss: 0.553\n",
      "[2, 23300] loss: 0.420\n",
      "[2, 23400] loss: 0.449\n",
      "[2, 23500] loss: 0.546\n",
      "[2, 23600] loss: 0.695\n",
      "[2, 23700] loss: 0.503\n",
      "[2, 23800] loss: 0.411\n",
      "[2, 23900] loss: 0.528\n",
      "[2, 24000] loss: 0.406\n",
      "[2, 24100] loss: 0.409\n",
      "[2, 24200] loss: 0.597\n",
      "[2, 24300] loss: 0.564\n",
      "[2, 24400] loss: 0.591\n",
      "[2, 24500] loss: 0.391\n",
      "[2, 24600] loss: 0.589\n",
      "[2, 24700] loss: 0.541\n",
      "[2, 24800] loss: 0.525\n",
      "[2, 24900] loss: 0.454\n",
      "[2, 25000] loss: 0.433\n",
      "[2, 25100] loss: 0.642\n",
      "[2, 25200] loss: 0.582\n",
      "[2, 25300] loss: 0.528\n",
      "[2, 25400] loss: 0.332\n",
      "[2, 25500] loss: 0.528\n",
      "[2, 25600] loss: 0.610\n",
      "[2, 25700] loss: 0.601\n",
      "[2, 25800] loss: 0.380\n",
      "[2, 25900] loss: 0.471\n",
      "[2, 26000] loss: 0.430\n",
      "[2, 26100] loss: 0.503\n",
      "[2, 26200] loss: 0.568\n",
      "[2, 26300] loss: 0.520\n",
      "[2, 26400] loss: 0.537\n",
      "[2, 26500] loss: 0.424\n",
      "[2, 26600] loss: 0.474\n",
      "[2, 26700] loss: 0.392\n",
      "[2, 26800] loss: 0.667\n",
      "[2, 26900] loss: 0.549\n",
      "[2, 27000] loss: 0.396\n",
      "[2, 27100] loss: 0.413\n",
      "[2, 27200] loss: 0.570\n",
      "[2, 27300] loss: 0.501\n",
      "[2, 27400] loss: 0.496\n",
      "[2, 27500] loss: 0.510\n",
      "[2, 27600] loss: 0.438\n",
      "[2, 27700] loss: 0.479\n",
      "[2, 27800] loss: 0.565\n",
      "[2, 27900] loss: 0.389\n",
      "[2, 28000] loss: 0.547\n",
      "[2, 28100] loss: 0.490\n",
      "[2, 28200] loss: 0.408\n",
      "[2, 28300] loss: 0.490\n",
      "[2, 28400] loss: 0.431\n",
      "[2, 28500] loss: 0.458\n",
      "[2, 28600] loss: 0.598\n",
      "[2, 28700] loss: 0.600\n",
      "[2, 28800] loss: 0.546\n",
      "[2, 28900] loss: 0.529\n",
      "[2, 29000] loss: 0.414\n",
      "[2, 29100] loss: 0.469\n",
      "[2, 29200] loss: 0.589\n",
      "[2, 29300] loss: 0.519\n",
      "[2, 29400] loss: 0.387\n",
      "[2, 29500] loss: 0.416\n",
      "[2, 29600] loss: 0.526\n",
      "[2, 29700] loss: 0.584\n",
      "[2, 29800] loss: 0.632\n",
      "[2, 29900] loss: 0.423\n",
      "[2, 30000] loss: 0.670\n",
      "[2, 30100] loss: 0.411\n",
      "[2, 30200] loss: 0.402\n",
      "[2, 30300] loss: 0.612\n",
      "[2, 30400] loss: 0.510\n",
      "[2, 30500] loss: 0.405\n",
      "[2, 30600] loss: 0.425\n",
      "[2, 30700] loss: 0.410\n",
      "[2, 30800] loss: 0.507\n",
      "[2, 30900] loss: 0.594\n",
      "[2, 31000] loss: 0.438\n",
      "[2, 31100] loss: 0.573\n",
      "[2, 31200] loss: 0.560\n",
      "[2, 31300] loss: 0.378\n",
      "[2, 31400] loss: 0.471\n",
      "[2, 31500] loss: 0.396\n",
      "[2, 31600] loss: 0.440\n",
      "[2, 31700] loss: 0.584\n",
      "[2, 31800] loss: 0.500\n",
      "[2, 31900] loss: 0.400\n",
      "[2, 32000] loss: 0.536\n",
      "[2, 32100] loss: 0.523\n",
      "[2, 32200] loss: 0.525\n",
      "[2, 32300] loss: 0.586\n",
      "[2, 32400] loss: 0.495\n",
      "[2, 32500] loss: 0.544\n",
      "[2, 32600] loss: 0.481\n",
      "[2, 32700] loss: 0.532\n",
      "[2, 32800] loss: 0.559\n",
      "[2, 32900] loss: 0.445\n",
      "[2, 33000] loss: 0.476\n",
      "[2, 33100] loss: 0.609\n",
      "[2, 33200] loss: 0.633\n",
      "[2, 33300] loss: 0.487\n",
      "[2, 33400] loss: 0.487\n",
      "[2, 33500] loss: 0.487\n",
      "[2, 33600] loss: 0.654\n",
      "[2, 33700] loss: 0.401\n",
      "[2, 33800] loss: 0.556\n",
      "[2, 33900] loss: 0.527\n",
      "[2, 34000] loss: 0.494\n",
      "[2, 34100] loss: 0.522\n",
      "[2, 34200] loss: 0.547\n",
      "[2, 34300] loss: 0.361\n",
      "[2, 34400] loss: 0.449\n",
      "[2, 34500] loss: 0.456\n",
      "[2, 34600] loss: 0.634\n",
      "[2, 34700] loss: 0.438\n",
      "[2, 34800] loss: 0.505\n",
      "[2, 34900] loss: 0.508\n",
      "[2, 35000] loss: 0.427\n",
      "[2, 35100] loss: 0.439\n",
      "[2, 35200] loss: 0.384\n",
      "[2, 35300] loss: 0.401\n",
      "[2, 35400] loss: 0.644\n",
      "[2, 35500] loss: 0.542\n",
      "[2, 35600] loss: 0.567\n",
      "[2, 35700] loss: 0.372\n",
      "[2, 35800] loss: 0.393\n",
      "[2, 35900] loss: 0.511\n",
      "[2, 36000] loss: 0.554\n",
      "[2, 36100] loss: 0.463\n",
      "[2, 36200] loss: 0.431\n",
      "[2, 36300] loss: 0.465\n",
      "[2, 36400] loss: 0.591\n",
      "[2, 36500] loss: 0.438\n",
      "[2, 36600] loss: 0.616\n",
      "[2, 36700] loss: 0.443\n",
      "[2, 36800] loss: 0.456\n",
      "[2, 36900] loss: 0.424\n",
      "[2, 37000] loss: 0.528\n",
      "[2, 37100] loss: 0.402\n",
      "[2, 37200] loss: 0.546\n",
      "[2, 37300] loss: 0.520\n",
      "[2, 37400] loss: 0.328\n",
      "[2, 37500] loss: 0.568\n",
      "[2, 37600] loss: 0.622\n",
      "[2, 37700] loss: 0.555\n",
      "[2, 37800] loss: 0.575\n",
      "[2, 37900] loss: 0.349\n",
      "[2, 38000] loss: 0.565\n",
      "[2, 38100] loss: 0.380\n",
      "[2, 38200] loss: 0.575\n",
      "[2, 38300] loss: 0.577\n",
      "[2, 38400] loss: 0.563\n",
      "[2, 38500] loss: 0.557\n",
      "[2, 38600] loss: 0.459\n",
      "[2, 38700] loss: 0.376\n",
      "[2, 38800] loss: 0.398\n",
      "[2, 38900] loss: 0.473\n",
      "[2, 39000] loss: 0.571\n",
      "[2, 39100] loss: 0.408\n",
      "[2, 39200] loss: 0.590\n",
      "[2, 39300] loss: 0.291\n",
      "[2, 39400] loss: 0.487\n",
      "[2, 39500] loss: 0.551\n",
      "[2, 39600] loss: 0.380\n",
      "[2, 39700] loss: 0.455\n",
      "[2, 39800] loss: 0.546\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d228d186a345d7a4c610dcf0a03e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100] loss: 0.494\n",
      "[3, 200] loss: 0.457\n",
      "[3, 300] loss: 0.610\n",
      "[3, 400] loss: 0.586\n",
      "[3, 500] loss: 0.535\n",
      "[3, 600] loss: 0.417\n",
      "[3, 700] loss: 0.369\n",
      "[3, 800] loss: 0.496\n",
      "[3, 900] loss: 0.457\n",
      "[3, 1000] loss: 0.428\n",
      "[3, 1100] loss: 0.582\n",
      "[3, 1200] loss: 0.571\n",
      "[3, 1300] loss: 0.542\n",
      "[3, 1400] loss: 0.561\n",
      "[3, 1500] loss: 0.520\n",
      "[3, 1600] loss: 0.418\n",
      "[3, 1700] loss: 0.447\n",
      "[3, 1800] loss: 0.524\n",
      "[3, 1900] loss: 0.420\n",
      "[3, 2000] loss: 0.458\n",
      "[3, 2100] loss: 0.522\n",
      "[3, 2200] loss: 0.475\n",
      "[3, 2300] loss: 0.342\n",
      "[3, 2400] loss: 0.583\n",
      "[3, 2500] loss: 0.514\n",
      "[3, 2600] loss: 0.523\n",
      "[3, 2700] loss: 0.483\n",
      "[3, 2800] loss: 0.516\n",
      "[3, 2900] loss: 0.460\n",
      "[3, 3000] loss: 0.469\n",
      "[3, 3100] loss: 0.496\n",
      "[3, 3200] loss: 0.442\n",
      "[3, 3300] loss: 0.493\n",
      "[3, 3400] loss: 0.572\n",
      "[3, 3500] loss: 0.542\n",
      "[3, 3600] loss: 0.491\n",
      "[3, 3700] loss: 0.506\n",
      "[3, 3800] loss: 0.527\n",
      "[3, 3900] loss: 0.556\n",
      "[3, 4000] loss: 0.418\n",
      "[3, 4100] loss: 0.483\n",
      "[3, 4200] loss: 0.389\n",
      "[3, 4300] loss: 0.595\n",
      "[3, 4400] loss: 0.524\n",
      "[3, 4500] loss: 0.498\n",
      "[3, 4600] loss: 0.415\n",
      "[3, 4700] loss: 0.597\n",
      "[3, 4800] loss: 0.647\n",
      "[3, 4900] loss: 0.317\n",
      "[3, 5000] loss: 0.404\n",
      "[3, 5100] loss: 0.560\n",
      "[3, 5200] loss: 0.435\n",
      "[3, 5300] loss: 0.324\n",
      "[3, 5400] loss: 0.429\n",
      "[3, 5500] loss: 0.449\n",
      "[3, 5600] loss: 0.521\n",
      "[3, 5700] loss: 0.558\n",
      "[3, 5800] loss: 0.380\n",
      "[3, 5900] loss: 0.478\n",
      "[3, 6000] loss: 0.500\n",
      "[3, 6100] loss: 0.441\n",
      "[3, 6200] loss: 0.350\n",
      "[3, 6300] loss: 0.447\n",
      "[3, 6400] loss: 0.565\n",
      "[3, 6500] loss: 0.483\n",
      "[3, 6600] loss: 0.535\n",
      "[3, 6700] loss: 0.517\n",
      "[3, 6800] loss: 0.465\n",
      "[3, 6900] loss: 0.343\n",
      "[3, 7000] loss: 0.449\n",
      "[3, 7100] loss: 0.410\n",
      "[3, 7200] loss: 0.502\n",
      "[3, 7300] loss: 0.477\n",
      "[3, 7400] loss: 0.340\n",
      "[3, 7500] loss: 0.424\n",
      "[3, 7600] loss: 0.555\n",
      "[3, 7700] loss: 0.448\n",
      "[3, 7800] loss: 0.520\n",
      "[3, 7900] loss: 0.511\n",
      "[3, 8000] loss: 0.554\n",
      "[3, 8100] loss: 0.426\n",
      "[3, 8200] loss: 0.392\n",
      "[3, 8300] loss: 0.559\n",
      "[3, 8400] loss: 0.554\n",
      "[3, 8500] loss: 0.593\n",
      "[3, 8600] loss: 0.366\n",
      "[3, 8700] loss: 0.455\n",
      "[3, 8800] loss: 0.498\n",
      "[3, 8900] loss: 0.493\n",
      "[3, 9000] loss: 0.396\n",
      "[3, 9100] loss: 0.402\n",
      "[3, 9200] loss: 0.569\n",
      "[3, 9300] loss: 0.473\n",
      "[3, 9400] loss: 0.522\n",
      "[3, 9500] loss: 0.449\n",
      "[3, 9600] loss: 0.524\n",
      "[3, 9700] loss: 0.329\n",
      "[3, 9800] loss: 0.767\n",
      "[3, 9900] loss: 0.441\n",
      "[3, 10000] loss: 0.467\n",
      "[3, 10100] loss: 0.554\n",
      "[3, 10200] loss: 0.438\n",
      "[3, 10300] loss: 0.575\n",
      "[3, 10400] loss: 0.549\n",
      "[3, 10500] loss: 0.542\n",
      "[3, 10600] loss: 0.533\n",
      "[3, 10700] loss: 0.363\n",
      "[3, 10800] loss: 0.350\n",
      "[3, 10900] loss: 0.523\n",
      "[3, 11000] loss: 0.452\n",
      "[3, 11100] loss: 0.382\n",
      "[3, 11200] loss: 0.530\n",
      "[3, 11300] loss: 0.492\n",
      "[3, 11400] loss: 0.451\n",
      "[3, 11500] loss: 0.416\n",
      "[3, 11600] loss: 0.557\n",
      "[3, 11700] loss: 0.519\n",
      "[3, 11800] loss: 0.446\n",
      "[3, 11900] loss: 0.365\n",
      "[3, 12000] loss: 0.579\n",
      "[3, 12100] loss: 0.345\n",
      "[3, 12200] loss: 0.474\n",
      "[3, 12300] loss: 0.490\n",
      "[3, 12400] loss: 0.419\n",
      "[3, 12500] loss: 0.556\n",
      "[3, 12600] loss: 0.449\n",
      "[3, 12700] loss: 0.380\n",
      "[3, 12800] loss: 0.529\n",
      "[3, 12900] loss: 0.388\n",
      "[3, 13000] loss: 0.502\n",
      "[3, 13100] loss: 0.368\n",
      "[3, 13200] loss: 0.472\n",
      "[3, 13300] loss: 0.466\n",
      "[3, 13400] loss: 0.556\n",
      "[3, 13500] loss: 0.362\n",
      "[3, 13600] loss: 0.479\n",
      "[3, 13700] loss: 0.582\n",
      "[3, 13800] loss: 0.471\n",
      "[3, 13900] loss: 0.279\n",
      "[3, 14000] loss: 0.524\n",
      "[3, 14100] loss: 0.267\n",
      "[3, 14200] loss: 0.373\n",
      "[3, 14300] loss: 0.387\n",
      "[3, 14400] loss: 0.494\n",
      "[3, 14500] loss: 0.456\n",
      "[3, 14600] loss: 0.303\n",
      "[3, 14700] loss: 0.352\n",
      "[3, 14800] loss: 0.446\n",
      "[3, 14900] loss: 0.455\n",
      "[3, 15000] loss: 0.430\n",
      "[3, 15100] loss: 0.492\n",
      "[3, 15200] loss: 0.590\n",
      "[3, 15300] loss: 0.406\n",
      "[3, 15400] loss: 0.369\n",
      "[3, 15500] loss: 0.526\n",
      "[3, 15600] loss: 0.401\n",
      "[3, 15700] loss: 0.479\n",
      "[3, 15800] loss: 0.446\n",
      "[3, 15900] loss: 0.511\n",
      "[3, 16000] loss: 0.500\n",
      "[3, 16100] loss: 0.482\n",
      "[3, 16200] loss: 0.445\n",
      "[3, 16300] loss: 0.409\n",
      "[3, 16400] loss: 0.518\n",
      "[3, 16500] loss: 0.571\n",
      "[3, 16600] loss: 0.571\n",
      "[3, 16700] loss: 0.411\n",
      "[3, 16800] loss: 0.504\n",
      "[3, 16900] loss: 0.485\n",
      "[3, 17000] loss: 0.386\n",
      "[3, 17100] loss: 0.367\n",
      "[3, 17200] loss: 0.515\n",
      "[3, 17300] loss: 0.370\n",
      "[3, 17400] loss: 0.551\n",
      "[3, 17500] loss: 0.553\n",
      "[3, 17600] loss: 0.579\n",
      "[3, 17700] loss: 0.439\n",
      "[3, 17800] loss: 0.411\n",
      "[3, 17900] loss: 0.330\n",
      "[3, 18000] loss: 0.488\n",
      "[3, 18100] loss: 0.527\n",
      "[3, 18200] loss: 0.591\n",
      "[3, 18300] loss: 0.474\n",
      "[3, 18400] loss: 0.506\n",
      "[3, 18500] loss: 0.428\n",
      "[3, 18600] loss: 0.508\n",
      "[3, 18700] loss: 0.397\n",
      "[3, 18800] loss: 0.573\n",
      "[3, 18900] loss: 0.416\n",
      "[3, 19000] loss: 0.507\n",
      "[3, 19100] loss: 0.576\n",
      "[3, 19200] loss: 0.490\n",
      "[3, 19300] loss: 0.422\n",
      "[3, 19400] loss: 0.350\n",
      "[3, 19500] loss: 0.435\n",
      "[3, 19600] loss: 0.447\n",
      "[3, 19700] loss: 0.466\n",
      "[3, 19800] loss: 0.426\n",
      "[3, 19900] loss: 0.736\n",
      "[3, 20000] loss: 0.523\n",
      "[3, 20100] loss: 0.479\n",
      "[3, 20200] loss: 0.606\n",
      "[3, 20300] loss: 0.471\n",
      "[3, 20400] loss: 0.438\n",
      "[3, 20500] loss: 0.392\n",
      "[3, 20600] loss: 0.713\n",
      "[3, 20700] loss: 0.515\n",
      "[3, 20800] loss: 0.454\n",
      "[3, 20900] loss: 0.452\n",
      "[3, 21000] loss: 0.529\n",
      "[3, 21100] loss: 0.471\n",
      "[3, 21200] loss: 0.494\n",
      "[3, 21300] loss: 0.419\n",
      "[3, 21400] loss: 0.580\n",
      "[3, 21500] loss: 0.311\n",
      "[3, 21600] loss: 0.453\n",
      "[3, 21700] loss: 0.475\n",
      "[3, 21800] loss: 0.337\n",
      "[3, 21900] loss: 0.366\n",
      "[3, 22000] loss: 0.504\n",
      "[3, 22100] loss: 0.464\n",
      "[3, 22200] loss: 0.549\n",
      "[3, 22300] loss: 0.472\n",
      "[3, 22400] loss: 0.580\n",
      "[3, 22500] loss: 0.473\n",
      "[3, 22600] loss: 0.619\n",
      "[3, 22700] loss: 0.438\n",
      "[3, 22800] loss: 0.517\n",
      "[3, 22900] loss: 0.472\n",
      "[3, 23000] loss: 0.391\n",
      "[3, 23100] loss: 0.426\n",
      "[3, 23200] loss: 0.438\n",
      "[3, 23300] loss: 0.400\n",
      "[3, 23400] loss: 0.524\n",
      "[3, 23500] loss: 0.397\n",
      "[3, 23600] loss: 0.588\n",
      "[3, 23700] loss: 0.420\n",
      "[3, 23800] loss: 0.508\n",
      "[3, 23900] loss: 0.507\n",
      "[3, 24000] loss: 0.569\n",
      "[3, 24100] loss: 0.432\n",
      "[3, 24200] loss: 0.508\n",
      "[3, 24300] loss: 0.382\n",
      "[3, 24400] loss: 0.506\n",
      "[3, 24500] loss: 0.412\n",
      "[3, 24600] loss: 0.444\n",
      "[3, 24700] loss: 0.374\n",
      "[3, 24800] loss: 0.696\n",
      "[3, 24900] loss: 0.578\n",
      "[3, 25000] loss: 0.509\n",
      "[3, 25100] loss: 0.374\n",
      "[3, 25200] loss: 0.564\n",
      "[3, 25300] loss: 0.546\n",
      "[3, 25400] loss: 0.513\n",
      "[3, 25500] loss: 0.521\n",
      "[3, 25600] loss: 0.525\n",
      "[3, 25700] loss: 0.415\n",
      "[3, 25800] loss: 0.512\n",
      "[3, 25900] loss: 0.475\n",
      "[3, 26000] loss: 0.652\n",
      "[3, 26100] loss: 0.500\n",
      "[3, 26200] loss: 0.540\n",
      "[3, 26300] loss: 0.563\n",
      "[3, 26400] loss: 0.457\n",
      "[3, 26500] loss: 0.449\n",
      "[3, 26600] loss: 0.424\n",
      "[3, 26700] loss: 0.458\n",
      "[3, 26800] loss: 0.495\n",
      "[3, 26900] loss: 0.498\n",
      "[3, 27000] loss: 0.701\n",
      "[3, 27100] loss: 0.529\n",
      "[3, 27200] loss: 0.364\n",
      "[3, 27300] loss: 0.462\n",
      "[3, 27400] loss: 0.591\n",
      "[3, 27500] loss: 0.574\n",
      "[3, 27600] loss: 0.387\n",
      "[3, 27700] loss: 0.561\n",
      "[3, 27800] loss: 0.505\n",
      "[3, 27900] loss: 0.395\n",
      "[3, 28000] loss: 0.495\n",
      "[3, 28100] loss: 0.486\n",
      "[3, 28200] loss: 0.539\n",
      "[3, 28300] loss: 0.491\n",
      "[3, 28400] loss: 0.579\n",
      "[3, 28500] loss: 0.394\n",
      "[3, 28600] loss: 0.435\n",
      "[3, 28700] loss: 0.429\n",
      "[3, 28800] loss: 0.450\n",
      "[3, 28900] loss: 0.533\n",
      "[3, 29000] loss: 0.467\n",
      "[3, 29100] loss: 0.521\n",
      "[3, 29200] loss: 0.520\n",
      "[3, 29300] loss: 0.551\n",
      "[3, 29400] loss: 0.464\n",
      "[3, 29500] loss: 0.441\n",
      "[3, 29600] loss: 0.428\n",
      "[3, 29700] loss: 0.441\n",
      "[3, 29800] loss: 0.514\n",
      "[3, 29900] loss: 0.554\n",
      "[3, 30000] loss: 0.401\n",
      "[3, 30100] loss: 0.511\n",
      "[3, 30200] loss: 0.320\n",
      "[3, 30300] loss: 0.491\n",
      "[3, 30400] loss: 0.524\n",
      "[3, 30500] loss: 0.483\n",
      "[3, 30600] loss: 0.467\n",
      "[3, 30700] loss: 0.461\n",
      "[3, 30800] loss: 0.625\n",
      "[3, 30900] loss: 0.457\n",
      "[3, 31000] loss: 0.357\n",
      "[3, 31100] loss: 0.446\n",
      "[3, 31200] loss: 0.620\n",
      "[3, 31300] loss: 0.365\n",
      "[3, 31400] loss: 0.482\n",
      "[3, 31500] loss: 0.507\n",
      "[3, 31600] loss: 0.328\n",
      "[3, 31700] loss: 0.402\n",
      "[3, 31800] loss: 0.498\n",
      "[3, 31900] loss: 0.422\n",
      "[3, 32000] loss: 0.546\n",
      "[3, 32100] loss: 0.573\n",
      "[3, 32200] loss: 0.417\n",
      "[3, 32300] loss: 0.556\n",
      "[3, 32400] loss: 0.428\n",
      "[3, 32500] loss: 0.563\n",
      "[3, 32600] loss: 0.521\n",
      "[3, 32700] loss: 0.524\n",
      "[3, 32800] loss: 0.592\n",
      "[3, 32900] loss: 0.472\n",
      "[3, 33000] loss: 0.432\n",
      "[3, 33100] loss: 0.507\n",
      "[3, 33200] loss: 0.424\n",
      "[3, 33300] loss: 0.431\n",
      "[3, 33400] loss: 0.417\n",
      "[3, 33500] loss: 0.515\n",
      "[3, 33600] loss: 0.628\n",
      "[3, 33700] loss: 0.511\n",
      "[3, 33800] loss: 0.530\n",
      "[3, 33900] loss: 0.417\n",
      "[3, 34000] loss: 0.438\n",
      "[3, 34100] loss: 0.541\n",
      "[3, 34200] loss: 0.426\n",
      "[3, 34300] loss: 0.430\n",
      "[3, 34400] loss: 0.505\n",
      "[3, 34500] loss: 0.485\n",
      "[3, 34600] loss: 0.505\n",
      "[3, 34700] loss: 0.677\n",
      "[3, 34800] loss: 0.594\n",
      "[3, 34900] loss: 0.462\n",
      "[3, 35000] loss: 0.505\n",
      "[3, 35100] loss: 0.394\n",
      "[3, 35200] loss: 0.408\n",
      "[3, 35300] loss: 0.519\n",
      "[3, 35400] loss: 0.508\n",
      "[3, 35500] loss: 0.576\n",
      "[3, 35600] loss: 0.515\n",
      "[3, 35700] loss: 0.472\n",
      "[3, 35800] loss: 0.378\n",
      "[3, 35900] loss: 0.472\n",
      "[3, 36000] loss: 0.556\n",
      "[3, 36100] loss: 0.507\n",
      "[3, 36200] loss: 0.474\n",
      "[3, 36300] loss: 0.463\n",
      "[3, 36400] loss: 0.399\n",
      "[3, 36500] loss: 0.453\n",
      "[3, 36600] loss: 0.473\n",
      "[3, 36700] loss: 0.472\n",
      "[3, 36800] loss: 0.422\n",
      "[3, 36900] loss: 0.490\n",
      "[3, 37000] loss: 0.477\n",
      "[3, 37100] loss: 0.456\n",
      "[3, 37200] loss: 0.541\n",
      "[3, 37300] loss: 0.463\n",
      "[3, 37400] loss: 0.476\n",
      "[3, 37500] loss: 0.533\n",
      "[3, 37600] loss: 0.615\n",
      "[3, 37700] loss: 0.430\n",
      "[3, 37800] loss: 0.467\n",
      "[3, 37900] loss: 0.554\n",
      "[3, 38000] loss: 0.473\n",
      "[3, 38100] loss: 0.461\n",
      "[3, 38200] loss: 0.552\n",
      "[3, 38300] loss: 0.454\n",
      "[3, 38400] loss: 0.407\n",
      "[3, 38500] loss: 0.529\n",
      "[3, 38600] loss: 0.517\n",
      "[3, 38700] loss: 0.507\n",
      "[3, 38800] loss: 0.393\n",
      "[3, 38900] loss: 0.443\n",
      "[3, 39000] loss: 0.621\n",
      "[3, 39100] loss: 0.544\n",
      "[3, 39200] loss: 0.401\n",
      "[3, 39300] loss: 0.330\n",
      "[3, 39400] loss: 0.438\n",
      "[3, 39500] loss: 0.556\n",
      "[3, 39600] loss: 0.583\n",
      "[3, 39700] loss: 0.530\n",
      "[3, 39800] loss: 0.442\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7929253981f548fcaadfe23082349765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 100] loss: 0.521\n",
      "[4, 200] loss: 0.414\n",
      "[4, 300] loss: 0.314\n",
      "[4, 400] loss: 0.470\n",
      "[4, 500] loss: 0.370\n",
      "[4, 600] loss: 0.507\n",
      "[4, 700] loss: 0.385\n",
      "[4, 800] loss: 0.459\n",
      "[4, 900] loss: 0.389\n",
      "[4, 1000] loss: 0.454\n",
      "[4, 1100] loss: 0.506\n",
      "[4, 1200] loss: 0.480\n",
      "[4, 1300] loss: 0.497\n",
      "[4, 1400] loss: 0.418\n",
      "[4, 1500] loss: 0.343\n",
      "[4, 1600] loss: 0.365\n",
      "[4, 1700] loss: 0.454\n",
      "[4, 1800] loss: 0.473\n",
      "[4, 1900] loss: 0.721\n",
      "[4, 2000] loss: 0.481\n",
      "[4, 2100] loss: 0.413\n",
      "[4, 2200] loss: 0.480\n",
      "[4, 2300] loss: 0.558\n",
      "[4, 2400] loss: 0.423\n",
      "[4, 2500] loss: 0.339\n",
      "[4, 2600] loss: 0.412\n",
      "[4, 2700] loss: 0.527\n",
      "[4, 2800] loss: 0.576\n",
      "[4, 2900] loss: 0.481\n",
      "[4, 3000] loss: 0.417\n",
      "[4, 3100] loss: 0.523\n",
      "[4, 3200] loss: 0.449\n",
      "[4, 3300] loss: 0.432\n",
      "[4, 3400] loss: 0.399\n",
      "[4, 3500] loss: 0.418\n",
      "[4, 3600] loss: 0.498\n",
      "[4, 3700] loss: 0.488\n",
      "[4, 3800] loss: 0.461\n",
      "[4, 3900] loss: 0.402\n",
      "[4, 4000] loss: 0.626\n",
      "[4, 4100] loss: 0.408\n",
      "[4, 4200] loss: 0.384\n",
      "[4, 4300] loss: 0.435\n",
      "[4, 4400] loss: 0.354\n",
      "[4, 4500] loss: 0.383\n",
      "[4, 4600] loss: 0.422\n",
      "[4, 4700] loss: 0.459\n",
      "[4, 4800] loss: 0.395\n",
      "[4, 4900] loss: 0.516\n",
      "[4, 5000] loss: 0.272\n",
      "[4, 5100] loss: 0.579\n",
      "[4, 5200] loss: 0.595\n",
      "[4, 5300] loss: 0.697\n",
      "[4, 5400] loss: 0.514\n",
      "[4, 5500] loss: 0.410\n",
      "[4, 5600] loss: 0.620\n",
      "[4, 5700] loss: 0.321\n",
      "[4, 5800] loss: 0.409\n",
      "[4, 5900] loss: 0.454\n",
      "[4, 6000] loss: 0.425\n",
      "[4, 6100] loss: 0.422\n",
      "[4, 6200] loss: 0.408\n",
      "[4, 6300] loss: 0.515\n",
      "[4, 6400] loss: 0.369\n",
      "[4, 6500] loss: 0.731\n",
      "[4, 6600] loss: 0.536\n",
      "[4, 6700] loss: 0.457\n",
      "[4, 6800] loss: 0.348\n",
      "[4, 6900] loss: 0.537\n",
      "[4, 7000] loss: 0.408\n",
      "[4, 7100] loss: 0.525\n",
      "[4, 7200] loss: 0.462\n",
      "[4, 7300] loss: 0.343\n",
      "[4, 7400] loss: 0.340\n",
      "[4, 7500] loss: 0.369\n",
      "[4, 7600] loss: 0.638\n",
      "[4, 7700] loss: 0.486\n",
      "[4, 7800] loss: 0.360\n",
      "[4, 7900] loss: 0.509\n",
      "[4, 8000] loss: 0.482\n",
      "[4, 8100] loss: 0.438\n",
      "[4, 8200] loss: 0.446\n",
      "[4, 8300] loss: 0.439\n",
      "[4, 8400] loss: 0.408\n",
      "[4, 8500] loss: 0.535\n",
      "[4, 8600] loss: 0.418\n",
      "[4, 8700] loss: 0.654\n",
      "[4, 8800] loss: 0.622\n",
      "[4, 8900] loss: 0.409\n",
      "[4, 9000] loss: 0.398\n",
      "[4, 9100] loss: 0.469\n",
      "[4, 9200] loss: 0.551\n",
      "[4, 9300] loss: 0.463\n",
      "[4, 9400] loss: 0.490\n",
      "[4, 9500] loss: 0.441\n",
      "[4, 9600] loss: 0.620\n",
      "[4, 9700] loss: 0.507\n",
      "[4, 9800] loss: 0.410\n",
      "[4, 9900] loss: 0.605\n",
      "[4, 10000] loss: 0.589\n",
      "[4, 10100] loss: 0.459\n",
      "[4, 10200] loss: 0.404\n",
      "[4, 10300] loss: 0.488\n",
      "[4, 10400] loss: 0.437\n",
      "[4, 10500] loss: 0.397\n",
      "[4, 10600] loss: 0.434\n",
      "[4, 10700] loss: 0.413\n",
      "[4, 10800] loss: 0.634\n",
      "[4, 10900] loss: 0.359\n",
      "[4, 11000] loss: 0.530\n",
      "[4, 11100] loss: 0.503\n",
      "[4, 11200] loss: 0.428\n",
      "[4, 11300] loss: 0.359\n",
      "[4, 11400] loss: 0.470\n",
      "[4, 11500] loss: 0.483\n",
      "[4, 11600] loss: 0.395\n",
      "[4, 11700] loss: 0.515\n",
      "[4, 11800] loss: 0.411\n",
      "[4, 11900] loss: 0.442\n",
      "[4, 12000] loss: 0.360\n",
      "[4, 12100] loss: 0.396\n",
      "[4, 12200] loss: 0.367\n",
      "[4, 12300] loss: 0.284\n",
      "[4, 12400] loss: 0.500\n",
      "[4, 12500] loss: 0.371\n",
      "[4, 12600] loss: 0.570\n",
      "[4, 12700] loss: 0.491\n",
      "[4, 12800] loss: 0.337\n",
      "[4, 12900] loss: 0.680\n",
      "[4, 13000] loss: 0.484\n",
      "[4, 13100] loss: 0.515\n",
      "[4, 13200] loss: 0.428\n",
      "[4, 13300] loss: 0.628\n",
      "[4, 13400] loss: 0.475\n",
      "[4, 13500] loss: 0.457\n",
      "[4, 13600] loss: 0.344\n",
      "[4, 13700] loss: 0.369\n",
      "[4, 13800] loss: 0.553\n",
      "[4, 13900] loss: 0.399\n",
      "[4, 14000] loss: 0.477\n",
      "[4, 14100] loss: 0.473\n",
      "[4, 14200] loss: 0.354\n",
      "[4, 14300] loss: 0.505\n",
      "[4, 14400] loss: 0.376\n",
      "[4, 14500] loss: 0.509\n",
      "[4, 14600] loss: 0.485\n",
      "[4, 14700] loss: 0.555\n",
      "[4, 14800] loss: 0.525\n",
      "[4, 14900] loss: 0.423\n",
      "[4, 15000] loss: 0.408\n",
      "[4, 15100] loss: 0.429\n",
      "[4, 15200] loss: 0.373\n",
      "[4, 15300] loss: 0.443\n",
      "[4, 15400] loss: 0.613\n",
      "[4, 15500] loss: 0.505\n",
      "[4, 15600] loss: 0.356\n",
      "[4, 15700] loss: 0.451\n",
      "[4, 15800] loss: 0.600\n",
      "[4, 15900] loss: 0.403\n",
      "[4, 16000] loss: 0.567\n",
      "[4, 16100] loss: 0.576\n",
      "[4, 16200] loss: 0.506\n",
      "[4, 16300] loss: 0.478\n",
      "[4, 16400] loss: 0.436\n",
      "[4, 16500] loss: 0.316\n",
      "[4, 16600] loss: 0.476\n",
      "[4, 16700] loss: 0.562\n",
      "[4, 16800] loss: 0.633\n",
      "[4, 16900] loss: 0.484\n",
      "[4, 17000] loss: 0.548\n",
      "[4, 17100] loss: 0.518\n",
      "[4, 17200] loss: 0.537\n",
      "[4, 17300] loss: 0.477\n",
      "[4, 17400] loss: 0.468\n",
      "[4, 17500] loss: 0.479\n",
      "[4, 17600] loss: 0.505\n",
      "[4, 17700] loss: 0.405\n",
      "[4, 17800] loss: 0.384\n",
      "[4, 17900] loss: 0.581\n",
      "[4, 18000] loss: 0.504\n",
      "[4, 18100] loss: 0.481\n",
      "[4, 18200] loss: 0.450\n",
      "[4, 18300] loss: 0.375\n",
      "[4, 18400] loss: 0.553\n",
      "[4, 18500] loss: 0.451\n",
      "[4, 18600] loss: 0.455\n",
      "[4, 18700] loss: 0.524\n",
      "[4, 18800] loss: 0.544\n",
      "[4, 18900] loss: 0.569\n",
      "[4, 19000] loss: 0.423\n",
      "[4, 19100] loss: 0.451\n",
      "[4, 19200] loss: 0.516\n",
      "[4, 19300] loss: 0.535\n",
      "[4, 19400] loss: 0.493\n",
      "[4, 19500] loss: 0.400\n",
      "[4, 19600] loss: 0.318\n",
      "[4, 19700] loss: 0.617\n",
      "[4, 19800] loss: 0.563\n",
      "[4, 19900] loss: 0.402\n",
      "[4, 20000] loss: 0.421\n",
      "[4, 20100] loss: 0.485\n",
      "[4, 20200] loss: 0.448\n",
      "[4, 20300] loss: 0.374\n",
      "[4, 20400] loss: 0.474\n",
      "[4, 20500] loss: 0.379\n",
      "[4, 20600] loss: 0.462\n",
      "[4, 20700] loss: 0.499\n",
      "[4, 20800] loss: 0.553\n",
      "[4, 20900] loss: 0.449\n",
      "[4, 21000] loss: 0.461\n",
      "[4, 21100] loss: 0.509\n",
      "[4, 21200] loss: 0.586\n",
      "[4, 21300] loss: 0.608\n",
      "[4, 21400] loss: 0.703\n",
      "[4, 21500] loss: 0.412\n",
      "[4, 21600] loss: 0.427\n",
      "[4, 21700] loss: 0.386\n",
      "[4, 21800] loss: 0.570\n",
      "[4, 21900] loss: 0.629\n",
      "[4, 22000] loss: 0.460\n",
      "[4, 22100] loss: 0.583\n",
      "[4, 22200] loss: 0.410\n",
      "[4, 22300] loss: 0.360\n",
      "[4, 22400] loss: 0.301\n",
      "[4, 22500] loss: 0.469\n",
      "[4, 22600] loss: 0.455\n",
      "[4, 22700] loss: 0.633\n",
      "[4, 22800] loss: 0.582\n",
      "[4, 22900] loss: 0.459\n",
      "[4, 23000] loss: 0.545\n",
      "[4, 23100] loss: 0.470\n",
      "[4, 23200] loss: 0.507\n",
      "[4, 23300] loss: 0.451\n",
      "[4, 23400] loss: 0.435\n",
      "[4, 23500] loss: 0.467\n",
      "[4, 23600] loss: 0.542\n",
      "[4, 23700] loss: 0.539\n",
      "[4, 23800] loss: 0.415\n",
      "[4, 23900] loss: 0.515\n",
      "[4, 24000] loss: 0.502\n",
      "[4, 24100] loss: 0.437\n",
      "[4, 24200] loss: 0.404\n",
      "[4, 24300] loss: 0.435\n",
      "[4, 24400] loss: 0.503\n",
      "[4, 24500] loss: 0.456\n",
      "[4, 24600] loss: 0.343\n",
      "[4, 24700] loss: 0.508\n",
      "[4, 24800] loss: 0.437\n",
      "[4, 24900] loss: 0.320\n",
      "[4, 25000] loss: 0.408\n",
      "[4, 25100] loss: 0.477\n",
      "[4, 25200] loss: 0.401\n",
      "[4, 25300] loss: 0.428\n",
      "[4, 25400] loss: 0.536\n",
      "[4, 25500] loss: 0.508\n",
      "[4, 25600] loss: 0.348\n",
      "[4, 25700] loss: 0.386\n",
      "[4, 25800] loss: 0.431\n",
      "[4, 25900] loss: 0.329\n",
      "[4, 26000] loss: 0.425\n",
      "[4, 26100] loss: 0.382\n",
      "[4, 26200] loss: 0.481\n",
      "[4, 26300] loss: 0.463\n",
      "[4, 26400] loss: 0.481\n",
      "[4, 26500] loss: 0.383\n",
      "[4, 26600] loss: 0.441\n",
      "[4, 26700] loss: 0.404\n",
      "[4, 26800] loss: 0.564\n",
      "[4, 26900] loss: 0.462\n",
      "[4, 27000] loss: 0.481\n",
      "[4, 27100] loss: 0.505\n",
      "[4, 27200] loss: 0.512\n",
      "[4, 27300] loss: 0.541\n",
      "[4, 27400] loss: 0.523\n",
      "[4, 27500] loss: 0.557\n",
      "[4, 27600] loss: 0.439\n",
      "[4, 27700] loss: 0.491\n",
      "[4, 27800] loss: 0.396\n",
      "[4, 27900] loss: 0.509\n",
      "[4, 28000] loss: 0.436\n",
      "[4, 28100] loss: 0.497\n",
      "[4, 28200] loss: 0.753\n",
      "[4, 28300] loss: 0.407\n",
      "[4, 28400] loss: 0.290\n",
      "[4, 28500] loss: 0.450\n",
      "[4, 28600] loss: 0.548\n",
      "[4, 28700] loss: 0.393\n",
      "[4, 28800] loss: 0.620\n",
      "[4, 28900] loss: 0.538\n",
      "[4, 29000] loss: 0.495\n",
      "[4, 29100] loss: 0.300\n",
      "[4, 29200] loss: 0.478\n",
      "[4, 29300] loss: 0.536\n",
      "[4, 29400] loss: 0.444\n",
      "[4, 29500] loss: 0.505\n",
      "[4, 29600] loss: 0.699\n",
      "[4, 29700] loss: 0.501\n",
      "[4, 29800] loss: 0.443\n",
      "[4, 29900] loss: 0.377\n",
      "[4, 30000] loss: 0.465\n",
      "[4, 30100] loss: 0.307\n",
      "[4, 30200] loss: 0.548\n",
      "[4, 30300] loss: 0.481\n",
      "[4, 30400] loss: 0.424\n",
      "[4, 30500] loss: 0.495\n",
      "[4, 30600] loss: 0.482\n",
      "[4, 30700] loss: 0.509\n",
      "[4, 30800] loss: 0.639\n",
      "[4, 30900] loss: 0.369\n",
      "[4, 31000] loss: 0.526\n",
      "[4, 31100] loss: 0.388\n",
      "[4, 31200] loss: 0.417\n",
      "[4, 31300] loss: 0.483\n",
      "[4, 31400] loss: 0.412\n",
      "[4, 31500] loss: 0.517\n",
      "[4, 31600] loss: 0.521\n",
      "[4, 31700] loss: 0.505\n",
      "[4, 31800] loss: 0.449\n",
      "[4, 31900] loss: 0.559\n",
      "[4, 32000] loss: 0.547\n",
      "[4, 32100] loss: 0.552\n",
      "[4, 32200] loss: 0.281\n",
      "[4, 32300] loss: 0.435\n",
      "[4, 32400] loss: 0.629\n",
      "[4, 32500] loss: 0.495\n",
      "[4, 32600] loss: 0.397\n",
      "[4, 32700] loss: 0.485\n",
      "[4, 32800] loss: 0.544\n",
      "[4, 32900] loss: 0.618\n",
      "[4, 33000] loss: 0.463\n",
      "[4, 33100] loss: 0.621\n",
      "[4, 33200] loss: 0.528\n",
      "[4, 33300] loss: 0.516\n",
      "[4, 33400] loss: 0.501\n",
      "[4, 33500] loss: 0.462\n",
      "[4, 33600] loss: 0.450\n",
      "[4, 33700] loss: 0.505\n",
      "[4, 33800] loss: 0.511\n",
      "[4, 33900] loss: 0.588\n",
      "[4, 34000] loss: 0.563\n",
      "[4, 34100] loss: 0.483\n",
      "[4, 34200] loss: 0.477\n",
      "[4, 34300] loss: 0.477\n",
      "[4, 34400] loss: 0.568\n",
      "[4, 34500] loss: 0.412\n",
      "[4, 34600] loss: 0.594\n",
      "[4, 34700] loss: 0.465\n",
      "[4, 34800] loss: 0.491\n",
      "[4, 34900] loss: 0.429\n",
      "[4, 35000] loss: 0.548\n",
      "[4, 35100] loss: 0.476\n",
      "[4, 35200] loss: 0.470\n",
      "[4, 35300] loss: 0.550\n",
      "[4, 35400] loss: 0.522\n",
      "[4, 35500] loss: 0.518\n",
      "[4, 35600] loss: 0.576\n",
      "[4, 35700] loss: 0.449\n",
      "[4, 35800] loss: 0.427\n",
      "[4, 35900] loss: 0.451\n",
      "[4, 36000] loss: 0.496\n",
      "[4, 36100] loss: 0.354\n",
      "[4, 36200] loss: 0.526\n",
      "[4, 36300] loss: 0.533\n",
      "[4, 36400] loss: 0.520\n",
      "[4, 36500] loss: 0.387\n",
      "[4, 36600] loss: 0.604\n",
      "[4, 36700] loss: 0.538\n",
      "[4, 36800] loss: 0.440\n",
      "[4, 36900] loss: 0.464\n",
      "[4, 37000] loss: 0.671\n",
      "[4, 37100] loss: 0.533\n",
      "[4, 37200] loss: 0.405\n",
      "[4, 37300] loss: 0.541\n",
      "[4, 37400] loss: 0.475\n",
      "[4, 37500] loss: 0.477\n",
      "[4, 37600] loss: 0.498\n",
      "[4, 37700] loss: 0.402\n",
      "[4, 37800] loss: 0.492\n",
      "[4, 37900] loss: 0.467\n",
      "[4, 38000] loss: 0.391\n",
      "[4, 38100] loss: 0.508\n",
      "[4, 38200] loss: 0.426\n",
      "[4, 38300] loss: 0.528\n",
      "[4, 38400] loss: 0.630\n",
      "[4, 38500] loss: 0.431\n",
      "[4, 38600] loss: 0.606\n",
      "[4, 38700] loss: 0.658\n",
      "[4, 38800] loss: 0.378\n",
      "[4, 38900] loss: 0.584\n",
      "[4, 39000] loss: 0.531\n",
      "[4, 39100] loss: 0.498\n",
      "[4, 39200] loss: 0.461\n",
      "[4, 39300] loss: 0.387\n",
      "[4, 39400] loss: 0.577\n",
      "[4, 39500] loss: 0.508\n",
      "[4, 39600] loss: 0.590\n",
      "[4, 39700] loss: 0.484\n",
      "[4, 39800] loss: 0.591\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839417033baf471f825d890a4271f397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39893), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 100] loss: 0.466\n",
      "[5, 200] loss: 0.405\n",
      "[5, 300] loss: 0.553\n",
      "[5, 400] loss: 0.418\n",
      "[5, 500] loss: 0.459\n",
      "[5, 600] loss: 0.462\n",
      "[5, 700] loss: 0.423\n",
      "[5, 800] loss: 0.417\n",
      "[5, 900] loss: 0.374\n",
      "[5, 1000] loss: 0.488\n",
      "[5, 1100] loss: 0.436\n",
      "[5, 1200] loss: 0.469\n",
      "[5, 1300] loss: 0.402\n",
      "[5, 1400] loss: 0.559\n",
      "[5, 1500] loss: 0.617\n",
      "[5, 1600] loss: 0.384\n",
      "[5, 1700] loss: 0.423\n",
      "[5, 1800] loss: 0.424\n",
      "[5, 1900] loss: 0.415\n",
      "[5, 2000] loss: 0.356\n",
      "[5, 2100] loss: 0.515\n",
      "[5, 2200] loss: 0.412\n",
      "[5, 2300] loss: 0.605\n",
      "[5, 2400] loss: 0.455\n",
      "[5, 2500] loss: 0.418\n",
      "[5, 2600] loss: 0.618\n",
      "[5, 2700] loss: 0.448\n",
      "[5, 2800] loss: 0.433\n",
      "[5, 2900] loss: 0.392\n",
      "[5, 3000] loss: 0.579\n",
      "[5, 3100] loss: 0.515\n",
      "[5, 3200] loss: 0.331\n",
      "[5, 3300] loss: 0.414\n",
      "[5, 3400] loss: 0.449\n",
      "[5, 3500] loss: 0.377\n",
      "[5, 3600] loss: 0.543\n",
      "[5, 3700] loss: 0.500\n",
      "[5, 3800] loss: 0.565\n",
      "[5, 3900] loss: 0.469\n",
      "[5, 4000] loss: 0.409\n",
      "[5, 4100] loss: 0.347\n",
      "[5, 4200] loss: 0.390\n",
      "[5, 4300] loss: 0.637\n",
      "[5, 4400] loss: 0.426\n",
      "[5, 4500] loss: 0.481\n",
      "[5, 4600] loss: 0.424\n",
      "[5, 4700] loss: 0.458\n",
      "[5, 4800] loss: 0.595\n",
      "[5, 4900] loss: 0.463\n",
      "[5, 5000] loss: 0.549\n",
      "[5, 5100] loss: 0.338\n",
      "[5, 5200] loss: 0.354\n",
      "[5, 5300] loss: 0.449\n",
      "[5, 5400] loss: 0.343\n",
      "[5, 5500] loss: 0.303\n",
      "[5, 5600] loss: 0.419\n",
      "[5, 5700] loss: 0.443\n",
      "[5, 5800] loss: 0.492\n",
      "[5, 5900] loss: 0.435\n",
      "[5, 6000] loss: 0.423\n",
      "[5, 6100] loss: 0.404\n",
      "[5, 6200] loss: 0.602\n",
      "[5, 6300] loss: 0.473\n",
      "[5, 6400] loss: 0.382\n",
      "[5, 6500] loss: 0.489\n",
      "[5, 6600] loss: 0.520\n",
      "[5, 6700] loss: 0.411\n",
      "[5, 6800] loss: 0.330\n",
      "[5, 6900] loss: 0.514\n",
      "[5, 7000] loss: 0.415\n",
      "[5, 7100] loss: 0.493\n",
      "[5, 7200] loss: 0.552\n",
      "[5, 7300] loss: 0.432\n",
      "[5, 7400] loss: 0.465\n",
      "[5, 7500] loss: 0.514\n",
      "[5, 7600] loss: 0.620\n",
      "[5, 7700] loss: 0.460\n",
      "[5, 7800] loss: 0.384\n",
      "[5, 7900] loss: 0.448\n",
      "[5, 8000] loss: 0.413\n",
      "[5, 8100] loss: 0.430\n",
      "[5, 8200] loss: 0.484\n",
      "[5, 8300] loss: 0.451\n",
      "[5, 8400] loss: 0.537\n",
      "[5, 8500] loss: 0.481\n",
      "[5, 8600] loss: 0.485\n",
      "[5, 8700] loss: 0.485\n",
      "[5, 8800] loss: 0.327\n",
      "[5, 8900] loss: 0.492\n",
      "[5, 9000] loss: 0.437\n",
      "[5, 9100] loss: 0.549\n",
      "[5, 9200] loss: 0.540\n",
      "[5, 9300] loss: 0.483\n",
      "[5, 9400] loss: 0.357\n",
      "[5, 9500] loss: 0.325\n",
      "[5, 9600] loss: 0.391\n",
      "[5, 9700] loss: 0.454\n",
      "[5, 9800] loss: 0.462\n",
      "[5, 9900] loss: 0.447\n",
      "[5, 10000] loss: 0.402\n",
      "[5, 10100] loss: 0.486\n",
      "[5, 10200] loss: 0.366\n",
      "[5, 10300] loss: 0.416\n",
      "[5, 10400] loss: 0.385\n",
      "[5, 10500] loss: 0.552\n",
      "[5, 10600] loss: 0.610\n",
      "[5, 10700] loss: 0.535\n",
      "[5, 10800] loss: 0.552\n",
      "[5, 10900] loss: 0.379\n",
      "[5, 11000] loss: 0.455\n",
      "[5, 11100] loss: 0.502\n",
      "[5, 11200] loss: 0.445\n",
      "[5, 11300] loss: 0.479\n",
      "[5, 11400] loss: 0.620\n",
      "[5, 11500] loss: 0.561\n",
      "[5, 11600] loss: 0.544\n",
      "[5, 11700] loss: 0.445\n",
      "[5, 11800] loss: 0.666\n",
      "[5, 11900] loss: 0.283\n",
      "[5, 12000] loss: 0.582\n",
      "[5, 12100] loss: 0.353\n",
      "[5, 12200] loss: 0.378\n",
      "[5, 12300] loss: 0.480\n",
      "[5, 12400] loss: 0.621\n",
      "[5, 12500] loss: 0.572\n",
      "[5, 12600] loss: 0.520\n",
      "[5, 12700] loss: 0.567\n",
      "[5, 12800] loss: 0.602\n",
      "[5, 12900] loss: 0.385\n",
      "[5, 13000] loss: 0.398\n",
      "[5, 13100] loss: 0.559\n",
      "[5, 13200] loss: 0.563\n",
      "[5, 13300] loss: 0.369\n",
      "[5, 13400] loss: 0.386\n",
      "[5, 13500] loss: 0.469\n",
      "[5, 13600] loss: 0.536\n",
      "[5, 13700] loss: 0.596\n",
      "[5, 13800] loss: 0.505\n",
      "[5, 13900] loss: 0.352\n",
      "[5, 14000] loss: 0.425\n",
      "[5, 14100] loss: 0.265\n",
      "[5, 14200] loss: 0.375\n",
      "[5, 14300] loss: 0.465\n",
      "[5, 14400] loss: 0.513\n",
      "[5, 14500] loss: 0.535\n",
      "[5, 14600] loss: 0.540\n",
      "[5, 14700] loss: 0.458\n",
      "[5, 14800] loss: 0.504\n",
      "[5, 14900] loss: 0.384\n",
      "[5, 15000] loss: 0.484\n",
      "[5, 15100] loss: 0.437\n",
      "[5, 15200] loss: 0.393\n",
      "[5, 15300] loss: 0.338\n",
      "[5, 15400] loss: 0.392\n",
      "[5, 15500] loss: 0.504\n",
      "[5, 15600] loss: 0.376\n",
      "[5, 15700] loss: 0.423\n",
      "[5, 15800] loss: 0.537\n",
      "[5, 15900] loss: 0.653\n",
      "[5, 16000] loss: 0.517\n",
      "[5, 16100] loss: 0.485\n",
      "[5, 16200] loss: 0.383\n",
      "[5, 16300] loss: 0.550\n",
      "[5, 16400] loss: 0.363\n",
      "[5, 16500] loss: 0.518\n",
      "[5, 16600] loss: 0.457\n",
      "[5, 16700] loss: 0.564\n",
      "[5, 16800] loss: 0.376\n",
      "[5, 16900] loss: 0.516\n",
      "[5, 17000] loss: 0.433\n",
      "[5, 17100] loss: 0.539\n",
      "[5, 17200] loss: 0.409\n",
      "[5, 17300] loss: 0.507\n",
      "[5, 17400] loss: 0.387\n",
      "[5, 17500] loss: 0.584\n",
      "[5, 17600] loss: 0.536\n",
      "[5, 17700] loss: 0.364\n",
      "[5, 17800] loss: 0.426\n",
      "[5, 17900] loss: 0.524\n",
      "[5, 18000] loss: 0.520\n",
      "[5, 18100] loss: 0.463\n",
      "[5, 18200] loss: 0.330\n",
      "[5, 18300] loss: 0.436\n",
      "[5, 18400] loss: 0.415\n",
      "[5, 18500] loss: 0.395\n",
      "[5, 18600] loss: 0.518\n",
      "[5, 18700] loss: 0.446\n",
      "[5, 18800] loss: 0.567\n",
      "[5, 18900] loss: 0.417\n",
      "[5, 19000] loss: 0.332\n",
      "[5, 19100] loss: 0.493\n",
      "[5, 19200] loss: 0.448\n",
      "[5, 19300] loss: 0.407\n",
      "[5, 19400] loss: 0.475\n",
      "[5, 19500] loss: 0.544\n",
      "[5, 19600] loss: 0.274\n",
      "[5, 19700] loss: 0.528\n",
      "[5, 19800] loss: 0.604\n",
      "[5, 19900] loss: 0.473\n",
      "[5, 20000] loss: 0.441\n",
      "[5, 20100] loss: 0.576\n",
      "[5, 20200] loss: 0.486\n",
      "[5, 20300] loss: 0.464\n",
      "[5, 20400] loss: 0.498\n",
      "[5, 20500] loss: 0.402\n",
      "[5, 20600] loss: 0.444\n",
      "[5, 20700] loss: 0.559\n",
      "[5, 20800] loss: 0.431\n",
      "[5, 20900] loss: 0.661\n",
      "[5, 21000] loss: 0.428\n",
      "[5, 21100] loss: 0.439\n",
      "[5, 21200] loss: 0.407\n",
      "[5, 21300] loss: 0.408\n",
      "[5, 21400] loss: 0.455\n",
      "[5, 21500] loss: 0.542\n",
      "[5, 21600] loss: 0.532\n",
      "[5, 21700] loss: 0.333\n",
      "[5, 21800] loss: 0.546\n",
      "[5, 21900] loss: 0.443\n",
      "[5, 22000] loss: 0.420\n",
      "[5, 22100] loss: 0.488\n",
      "[5, 22200] loss: 0.513\n",
      "[5, 22300] loss: 0.491\n",
      "[5, 22400] loss: 0.417\n",
      "[5, 22500] loss: 0.359\n",
      "[5, 22600] loss: 0.581\n",
      "[5, 22700] loss: 0.417\n",
      "[5, 22800] loss: 0.551\n",
      "[5, 22900] loss: 0.619\n",
      "[5, 23000] loss: 0.446\n",
      "[5, 23100] loss: 0.532\n",
      "[5, 23200] loss: 0.383\n",
      "[5, 23300] loss: 0.522\n",
      "[5, 23400] loss: 0.544\n",
      "[5, 23500] loss: 0.404\n",
      "[5, 23600] loss: 0.462\n",
      "[5, 23700] loss: 0.560\n",
      "[5, 23800] loss: 0.413\n",
      "[5, 23900] loss: 0.620\n",
      "[5, 24000] loss: 0.684\n",
      "[5, 24100] loss: 0.352\n",
      "[5, 24200] loss: 0.456\n",
      "[5, 24300] loss: 0.509\n",
      "[5, 24400] loss: 0.552\n",
      "[5, 24500] loss: 0.406\n",
      "[5, 24600] loss: 0.696\n",
      "[5, 24700] loss: 0.446\n",
      "[5, 24800] loss: 0.541\n",
      "[5, 24900] loss: 0.475\n",
      "[5, 25000] loss: 0.457\n",
      "[5, 25100] loss: 0.432\n",
      "[5, 25200] loss: 0.518\n",
      "[5, 25300] loss: 0.428\n",
      "[5, 25400] loss: 0.567\n",
      "[5, 25500] loss: 0.567\n",
      "[5, 25600] loss: 0.450\n",
      "[5, 25700] loss: 0.411\n",
      "[5, 25800] loss: 0.496\n",
      "[5, 25900] loss: 0.418\n",
      "[5, 26000] loss: 0.592\n",
      "[5, 26100] loss: 0.435\n",
      "[5, 26200] loss: 0.514\n",
      "[5, 26300] loss: 0.439\n",
      "[5, 26400] loss: 0.334\n",
      "[5, 26500] loss: 0.419\n",
      "[5, 26600] loss: 0.482\n",
      "[5, 26700] loss: 0.481\n",
      "[5, 26800] loss: 0.531\n",
      "[5, 26900] loss: 0.405\n",
      "[5, 27000] loss: 0.480\n",
      "[5, 27100] loss: 0.383\n",
      "[5, 27200] loss: 0.376\n",
      "[5, 27300] loss: 0.495\n",
      "[5, 27400] loss: 0.538\n",
      "[5, 27500] loss: 0.483\n",
      "[5, 27600] loss: 0.582\n",
      "[5, 27700] loss: 0.457\n",
      "[5, 27800] loss: 0.484\n",
      "[5, 27900] loss: 0.411\n",
      "[5, 28000] loss: 0.510\n",
      "[5, 28100] loss: 0.356\n",
      "[5, 28200] loss: 0.491\n",
      "[5, 28300] loss: 0.497\n",
      "[5, 28400] loss: 0.446\n",
      "[5, 28500] loss: 0.472\n",
      "[5, 28600] loss: 0.494\n",
      "[5, 28700] loss: 0.347\n",
      "[5, 28800] loss: 0.473\n",
      "[5, 28900] loss: 0.467\n",
      "[5, 29000] loss: 0.428\n",
      "[5, 29100] loss: 0.404\n",
      "[5, 29200] loss: 0.359\n",
      "[5, 29300] loss: 0.567\n",
      "[5, 29400] loss: 0.447\n",
      "[5, 29500] loss: 0.546\n",
      "[5, 29600] loss: 0.379\n",
      "[5, 29700] loss: 0.515\n",
      "[5, 29800] loss: 0.422\n",
      "[5, 29900] loss: 0.559\n",
      "[5, 30000] loss: 0.397\n",
      "[5, 30100] loss: 0.569\n",
      "[5, 30200] loss: 0.368\n",
      "[5, 30300] loss: 0.592\n",
      "[5, 30400] loss: 0.571\n",
      "[5, 30500] loss: 0.400\n",
      "[5, 30600] loss: 0.450\n",
      "[5, 30700] loss: 0.396\n",
      "[5, 30800] loss: 0.574\n",
      "[5, 30900] loss: 0.455\n",
      "[5, 31000] loss: 0.447\n",
      "[5, 31100] loss: 0.457\n",
      "[5, 31200] loss: 0.389\n",
      "[5, 31300] loss: 0.370\n",
      "[5, 31400] loss: 0.381\n",
      "[5, 31500] loss: 0.669\n",
      "[5, 31600] loss: 0.373\n",
      "[5, 31700] loss: 0.476\n",
      "[5, 31800] loss: 0.498\n",
      "[5, 31900] loss: 0.488\n",
      "[5, 32000] loss: 0.412\n",
      "[5, 32100] loss: 0.512\n",
      "[5, 32200] loss: 0.502\n",
      "[5, 32300] loss: 0.403\n",
      "[5, 32400] loss: 0.690\n",
      "[5, 32500] loss: 0.495\n",
      "[5, 32600] loss: 0.516\n",
      "[5, 32700] loss: 0.452\n",
      "[5, 32800] loss: 0.430\n",
      "[5, 32900] loss: 0.536\n",
      "[5, 33000] loss: 0.494\n",
      "[5, 33100] loss: 0.451\n",
      "[5, 33200] loss: 0.355\n",
      "[5, 33300] loss: 0.474\n",
      "[5, 33400] loss: 0.326\n",
      "[5, 33500] loss: 0.593\n",
      "[5, 33600] loss: 0.481\n",
      "[5, 33700] loss: 0.357\n",
      "[5, 33800] loss: 0.579\n",
      "[5, 33900] loss: 0.511\n",
      "[5, 34000] loss: 0.475\n",
      "[5, 34100] loss: 0.444\n",
      "[5, 34200] loss: 0.428\n",
      "[5, 34300] loss: 0.698\n",
      "[5, 34400] loss: 0.468\n",
      "[5, 34500] loss: 0.410\n",
      "[5, 34600] loss: 0.532\n",
      "[5, 34700] loss: 0.550\n",
      "[5, 34800] loss: 0.605\n",
      "[5, 34900] loss: 0.551\n",
      "[5, 35000] loss: 0.434\n",
      "[5, 35100] loss: 0.450\n",
      "[5, 35200] loss: 0.385\n",
      "[5, 35300] loss: 0.444\n",
      "[5, 35400] loss: 0.447\n",
      "[5, 35500] loss: 0.359\n",
      "[5, 35600] loss: 0.377\n",
      "[5, 35700] loss: 0.640\n",
      "[5, 35800] loss: 0.539\n",
      "[5, 35900] loss: 0.477\n",
      "[5, 36000] loss: 0.668\n",
      "[5, 36100] loss: 0.424\n",
      "[5, 36200] loss: 0.425\n",
      "[5, 36300] loss: 0.542\n",
      "[5, 36400] loss: 0.398\n",
      "[5, 36500] loss: 0.265\n",
      "[5, 36600] loss: 0.466\n",
      "[5, 36700] loss: 0.388\n",
      "[5, 36800] loss: 0.483\n",
      "[5, 36900] loss: 0.505\n",
      "[5, 37000] loss: 0.525\n",
      "[5, 37100] loss: 0.647\n",
      "[5, 37200] loss: 0.502\n",
      "[5, 37300] loss: 0.359\n",
      "[5, 37400] loss: 0.372\n",
      "[5, 37500] loss: 0.407\n",
      "[5, 37600] loss: 0.356\n",
      "[5, 37700] loss: 0.433\n",
      "[5, 37800] loss: 0.567\n",
      "[5, 37900] loss: 0.478\n",
      "[5, 38000] loss: 0.415\n",
      "[5, 38100] loss: 0.471\n",
      "[5, 38200] loss: 0.552\n",
      "[5, 38300] loss: 0.588\n",
      "[5, 38400] loss: 0.489\n",
      "[5, 38500] loss: 0.436\n",
      "[5, 38600] loss: 0.548\n",
      "[5, 38700] loss: 0.538\n",
      "[5, 38800] loss: 0.387\n",
      "[5, 38900] loss: 0.401\n",
      "[5, 39000] loss: 0.416\n",
      "[5, 39100] loss: 0.402\n",
      "[5, 39200] loss: 0.485\n",
      "[5, 39300] loss: 0.355\n",
      "[5, 39400] loss: 0.317\n",
      "[5, 39500] loss: 0.368\n",
      "[5, 39600] loss: 0.428\n",
      "[5, 39700] loss: 0.509\n",
      "[5, 39800] loss: 0.449\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 5\n",
    "batch_size = 4\n",
    "input_name = 'comment_text'\n",
    "label_names = ['toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "               'insult', 'identity_hate']\n",
    "\n",
    "batches_train, batches_val = data.BucketIterator.splits(\n",
    "    (train_set, val_set), batch_size=batch_size, repeat=False,\n",
    "    sort_key=lambda x: len(getattr(x, input_name)),\n",
    "    sort_within_batch=True)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(n_epochs):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for n, batch in enumerate(tqdm(batches_train)):\n",
    "        # get the inputs\n",
    "        inputs, lengths = getattr(batch, input_name)\n",
    "        lengths = lengths.cpu().numpy()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs, lengths)\n",
    "        losses = []\n",
    "    \n",
    "        # get all output fields and their losses\n",
    "        # and backprop for each of them\n",
    "        for i, label_name in enumerate(label_names):\n",
    "            targets = getattr(batch, label_name)\n",
    "            label_outputs = outputs[:, i]\n",
    "            loss = criterion(label_outputs, targets)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        total_loss = sum(losses)\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += total_loss.data[0]\n",
    "        if n % 100 == 99:    # print every 1000 mini-batches\n",
    "            print('[{}, {}] loss: {:.3f}'.format(epoch + 1,\n",
    "                                                n + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_val(net, val_iter, *, input_name, label_names):\n",
    "    Y_true = []\n",
    "    Y_hat = []\n",
    "    net.eval()\n",
    "    for batch in tqdm(val_iter):\n",
    "        X, lengths = getattr(batch, input_name)\n",
    "        lengths = lengths.cpu().numpy()\n",
    "        out = net(X, lengths)\n",
    "        out_probas = F.sigmoid(out)\n",
    "        \n",
    "        y_true_all = np.zeros(len(label_names), dtype='float32')\n",
    "        y_hat_all = np.zeros(len(label_names), dtype='float32')\n",
    "        for n, (label_name, label_proba) in enumerate(zip(label_names, out_probas)):\n",
    "            y_true = getattr(batch, label_name)\n",
    "            y_true_all[n] = y_true.cpu().tonumpy()\n",
    "            \n",
    "            y_hat_all[n] = label_proba\n",
    "    Y_true = np.vstack(Y_true)\n",
    "    Y_hat = np.vstack(Y_hat)\n",
    "    return Y_hat, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
